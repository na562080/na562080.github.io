<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>深度学习--Transformer | 梦游</title><meta name="author" content="沉云"><meta name="copyright" content="沉云"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习--Transformer">
<meta property="og:url" content="https://na562080.site/2025/08/05/deep-learning-Transformer.html">
<meta property="og:site_name" content="梦游">
<meta property="og:description" content="Transformer">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://na562080.site/img/avatar.webp">
<meta property="article:published_time" content="2025-08-05T08:50:32.000Z">
<meta property="article:modified_time" content="2025-08-05T13:36:27.000Z">
<meta property="article:author" content="沉云">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://na562080.site/img/avatar.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "深度学习--Transformer",
  "url": "https://na562080.site/2025/08/05/deep-learning-Transformer.html",
  "image": "https://na562080.site/img/avatar.webp",
  "datePublished": "2025-08-05T08:50:32.000Z",
  "dateModified": "2025-08-05T13:36:27.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "沉云",
      "url": "https://github.com/na562080"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://na562080.site/2025/08/05/deep-learning-Transformer.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"top_n_per_article":3,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习--Transformer',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/modify.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="梦游" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(/img/Clifftop_Walk_At_Pourville.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 画廊</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "/img/loading.gif" data-lazy-src="/img/avatar.webp" alt="Logo"><span class="site-name">梦游</span></a><a class="nav-page-title" href="/"><span class="site-name">深度学习--Transformer</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 画廊</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">深度学习--Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-08-05T08:50:32.000Z" title="发表于 2025-08-05 16:50:32">2025-08-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-05T13:36:27.000Z" title="更新于 2025-08-05 21:36:27">2025-08-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p>参考<br><a target="_blank" rel="noopener" href="https://zybuluo.com/hanbingtao/note/2600518">零基础入门深度学习(8) - Transformer (1&#x2F;3)</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/338817680">Transformer模型详解（图解）</a><br><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ugWDIIOHtPA&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=61">Transformer</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/42833949">resnet中的残差连接，你确定真的看懂了</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/687392131">信息瓶颈Information Bottleneck及信息论相关概念梳理</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42475060/article/details/121101749">一文读懂Transformer</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/benzhujie1245com/article/details/117173090?fromshare=blogdetail&sharetype=blogdetail&sharerId=117173090&sharerefer=PC&sharesource=2504_92570460&sharefrom=from_link">Transformer 模型详解</a></p>
</blockquote>
<blockquote>
<p>其他<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_63855028/article/details/146239674">自回归（Autoregressive）模型</a></p>
</blockquote>
<hr>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>Transformer的一些重要组成部分和特点：</p>
<ol>
<li>自注意力机制（Self-Attention）：Transformer的核心概念之一，它使模型能够同时考虑输入序列中的所有位置，而不是像循环神经网络（RNN）或卷积神经网络（CNN）一样逐步处理。自注意力机制允许模型根据输入序列中的不同部分来赋予不同的注意权重，从而更好地捕捉语义关系。<br>RNN不能并行的问题在于后一个输入状态依赖于前一个状态的输出，只能一个节点一个节点的慢慢算，transformer能并行计算是因为他前后没有依赖，比如Attention第一个单词在计算的时候其他单词也可以同时在计算，最终算出来的注意力矩阵一起输出到后面层</li>
<li>多头注意力（Multi-Head Attention）：Transformer中的自注意力机制被扩展为多个注意力头，每个头可以学习不同的注意权重，以更好地捕捉不同类型的关系。多头注意力允许模型并行处理不同的信息子空间。</li>
<li>堆叠层（Stacked Layers）：Transformer通常由多个相同的编码器和解码器层堆叠而成。这些堆叠的层有助于模型学习复杂的特征表示和语义。</li>
<li>位置编码（Positional Encoding）：由于Transformer没有内置的序列位置信息，它需要额外的位置编码来表达输入序列中单词的位置顺序。</li>
<li>残差连接和层归一化（Residual Connections and Layer Normalization）：减轻训练过程中的梯度消失和爆炸问题，使模型更容易训练。</li>
<li>编码器和解码器：Transformer通常包括一个编码器用于处理输入序列和一个解码器用于生成输出序列，这使其适用于序列到序列的任务，如机器翻译。</li>
</ol>
<p><img src= "/img/loading.gif" data-lazy-src="https://i-blog.csdnimg.cn/blog_migrate/50b27b7b0dfa4e53d66acd930cf0c103.png"></p>
<p>从编码器输入的句子首先会经过一个自注意力层，这一层帮助编码器在对每个单词编码的时候时刻关注句子的其它单词。</p>
<h2 id="嵌入层Embedding"><a href="#嵌入层Embedding" class="headerlink" title="嵌入层Embedding"></a>嵌入层Embedding</h2><blockquote>
<p>参考<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/164502624">一文读懂Embedding的概念，以及它和深度学习的关系</a></p>
</blockquote>
<p>每个 token 先被 embedding 成固定维度向量（d_model）。<br>Transformer 不采用 RNN 的结构，而是使用全局信息，不能利用单词的顺序信息。<br>注意力本身不包含位置信息，需要把位置编码（positional encoding）加到 embedding 上，常见的是正弦&#x2F;余弦形式或可学习的位置向量。</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://picx.zhimg.com/v2-6444601b4c41d99e70569b0ea388c3bd_1440w.jpg"></p>
<p>单射且同构的，稀疏矩阵做矩阵计算过度占用资源<br>嵌入层可以降维升维把一些其他特征给放大了，或者把笼统的特征给分开了</p>
<h2 id="自注意力机制（Self-Attention）"><a href="#自注意力机制（Self-Attention）" class="headerlink" title="自注意力机制（Self-Attention）"></a>自注意力机制（Self-Attention）</h2><p>核心是一个序列中的任意两个Token之间都要计算注意力权重，也就是每个Token都要注意自身以及序列中的其它Token。<br>注意力机制的核心，就是提升关键信息的权重，降低非关键信息的权重。</p>
<p>q, k, v分别是Query、Key、Value的缩写，它们是对注意力机制计算过程的抽象表达。三个权重矩阵都是模型训练过程中通过梯度下降自学习得到的参数</p>
<p>随着模型处理输入序列的每个单词，自注意力会关注整个输入序列的所有单词，帮助模型对本单词更好地进行编码。</p>
<h3 id="Scaled-Dot-Product-Attention（核心运算）"><a href="#Scaled-Dot-Product-Attention（核心运算）" class="headerlink" title="Scaled Dot-Product Attention（核心运算）"></a>Scaled Dot-Product Attention（核心运算）</h3><p>缩放的目的不是为了归一化概率，而是为了防止数值过大导致 softmax 的梯度消失问题。指数函数会让差异被极度放大，导致softmax 输出接近 one-hot（极端化），反向传播时梯度对大部分非最大值位置非常接近 0，训练不稳定、收敛慢。<br>用查询（Q）、键（K）、值（V）三组向量来计算注意力权重：注意力打分先算 Q 与 K 的点积、按 sqrt(d_k) 缩放，再做 softmax 得到权重，最后加权 V。每个 query 向量在所有 key 上打分，得到的权重决定从所有 value 中“聚合”出哪些信息。<br>对所有位置的信息进行加权求和，权重来自注意力分布</p>
<h3 id="Multi-Head-Attention（多头注意力）"><a href="#Multi-Head-Attention（多头注意力）" class="headerlink" title="Multi-Head Attention（多头注意力）"></a>Multi-Head Attention（多头注意力）</h3><p>为了让模型在不同的子空间并行学习不同的关联，Transformer 把 Q&#x2F;K&#x2F;V 线性投影成多个头（head），每个头单独做 attention，最后把各头的输出串接并再线性变换回原维度。多头能让模型同时关注不同类型的关系。<br><img src= "/img/loading.gif" data-lazy-src="https://picx.zhimg.com/v2-b0ea8f5b639786f98330f70405e94a75_1440w.jpg"></p>
<h2 id="Encoder-结构"><a href="#Encoder-结构" class="headerlink" title="Encoder 结构"></a>Encoder 结构</h2><h3 id="Add-Norm层"><a href="#Add-Norm层" class="headerlink" title="Add &amp; Norm层"></a>Add &amp; Norm层</h3><p>由 Add 和 Norm 两部分组成<br>其中 X表示 Multi-Head Attention 或者 Feed Forward 的输入，MultiHeadAttention(X) 和 FeedForward(X) 表示输出 (输出与输入 X 维度是一样的，所以可以相加)。<br><img src= "/img/loading.gif" data-lazy-src="https://pic1.zhimg.com/v2-a4b35db50f882522ee52f61ddd411a5a_1440w.jpg"><br>Add指 X+MultiHeadAttention(X)，是一种残差连接，通常用于解决多层网络训练的问题，可以让网络只关注当前差异的部分。<br><img src= "/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-4b3dde965124bd00f9893b05ebcaad0f_1440w.jpg"><br>Norm指 Layer Normalization，通常用于 RNN 结构，Layer Normalization 会将每一层神经元的输入都转成均值方差都一样的，这样可以加快收敛。</p>
<h3 id="前馈网络（Feed-Forward-Network-FFN）"><a href="#前馈网络（Feed-Forward-Network-FFN）" class="headerlink" title="前馈网络（Feed-Forward Network, FFN）"></a>前馈网络（Feed-Forward Network, FFN）</h3><p>扩大维度 + 非线性激活<br>是一个两层的全连接层，第一层的激活函数为 Relu，第二层不使用激活函数<br><img src= "/img/loading.gif" data-lazy-src="https://pic4.zhimg.com/v2-47b39ca4cc3cd0be157d6803c8c8e0a1_1440w.jpg"></p>
<p>FFN 对序列中每个位置独立地做“通道间”的非线性变换：把一个位置的向量投影到更高维度做非线性激活、再投回原位置。所以它增强每个位置内部的特征组合能力，补充了 attention 跨位置的信息聚合。</p>
<p>把维度升高有两层好处：1在高维空间里，线性分割面&#x2F;非线性组合有更多自由度，能组合出更丰富的特征（类似把数据映射到高维后更容易线性可分）。2实际上把原始通道做出许多“基方向”或“特征检测器”，激活后重新混合这些检测到的特征，产生更表达力强的输出。</p>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>通过上面描述的 Multi-Head Attention, Feed Forward, Add &amp; Norm 就可以构造出一个 Encoder block，Encoder block 接收输入矩阵，并输出一个矩阵。通过多个 Encoder block 叠加就可以组成 Encoder。<br>第一个 Encoder block 的输入为句子单词的表示向量矩阵，后续 Encoder block 的输入是前一个 Encoder block 的输出，最后一个 Encoder block 输出的矩阵就是编码信息矩阵 C，这一矩阵后续会用到 Decoder 中。</p>
<h2 id="Decoder-结构"><a href="#Decoder-结构" class="headerlink" title="Decoder 结构"></a>Decoder 结构</h2><h3 id="第一个-Multi-Head-Attention"><a href="#第一个-Multi-Head-Attention" class="headerlink" title="第一个 Multi-Head Attention"></a>第一个 Multi-Head Attention</h3><p>Decoder block 的第一个 Multi-Head Attention 采用了 Masked 操作，避免获取后面信息。 Mask 操作是在 Self-Attention 的 Softmax 之前使用的。</p>
<h3 id="第二个-Multi-Head-Attention"><a href="#第二个-Multi-Head-Attention" class="headerlink" title="第二个 Multi-Head Attention"></a>第二个 Multi-Head Attention</h3><p>主要的区别在于其中 Self-Attention 的 K, V矩阵不是使用 上一个 Decoder block 的输出计算的，而是使用 Encoder 的编码信息矩阵 C 计算的。<br>这样做的好处是在 Decoder 的时候，每一位单词都可以利用到 Encoder 所有单词的信息 (这些信息无需 Mask)。</p>
<h2 id="编码器-解码器的结构差异"><a href="#编码器-解码器的结构差异" class="headerlink" title="编码器 &#x2F; 解码器的结构差异"></a>编码器 &#x2F; 解码器的结构差异</h2><p><img src= "/img/loading.gif" data-lazy-src="https://i-blog.csdnimg.cn/blog_migrate/c1a6d1266e5339bf073b2ad75e7dc3b5.png"><br>Encoder（N 层堆叠）：每层包含 Multi-Head Self-Attention → Add&amp;Norm → FFN → Add&amp;Norm。</p>
<p>Decoder（N 层堆叠）：每层包含 Masked Multi-Head Self-Attention（防止看到未来信息）→ Add&amp;Norm → Encoder-Decoder Attention（把 encoder 输出作为 K,V）→ Add&amp;Norm → FFN → Add&amp;Norm。</p>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><p>训练&#x2F;前向（Encoder-Decoder，逐步）<br>Tokenize + Embedding：输入文本分词 → 每个 token 映射为向量（embedding）。</p>
<p>加位置编码：embedding + positional encoding。</p>
<p>送入 Encoder 堆叠：经过 L 个 encoder 层（self-attention + FFN + 残差 + LayerNorm），得到 encoder 输出序列（每个位置的上下文化向量）。<br>NeurIPS Papers</p>
<p>准备 Decoder 输入：训练时 decoder 可见目标序列（右移），推理时采用已生成的 token（自回归）。</p>
<p>Decoder 的 Masked Self-Attention：保证每个位置只看到当前及之前的输出（因果掩码）。<br>Stanford University</p>
<p>Encoder-Decoder Attention：decoder 的 queries 与 encoder 的 K,V 做 attention，从输入序列中检索相关信息。<br>NeurIPS Papers</p>
<p>最终线性 + softmax：decoder 最上层输出经过线性变换到词表维度并 softmax 得到下一个 token 的概率分布。</p>
<p>计算损失 &amp; 反向传播（训练）：用目标序列计算交叉熵，反向传播更新参数。</p>
<p>训练时：第i个decoder的输入 &#x3D; encoder输出 + ground truth embeding<br>预测时：第i个decoder的输入 &#x3D; encoder输出 + 第(i-1)个decoder输出<br>训练时因为知道ground truth embeding，相当于知道正确答案，网络可以一次训练完成。<br>预测时，首先输入start，输出预测的第一个单词 然后start和新单词组成新的query，再输入decoder来预测下一个单词，循环往复 直至end</p>
<p>Mask 在 Decoder Self-Attention 里完成，对应的是 序列内部的未来信息遮挡。</p>
<p>Encoder-Decoder Attention 不需要 Mask，因为 Encoder 输出是完整上下文，Decoder 可以自由查询。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/na562080">沉云</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://na562080.site/2025/08/05/deep-learning-Transformer.html">https://na562080.site/2025/08/05/deep-learning-Transformer.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://na562080.site" target="_blank">梦游</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer"><span class="toc-number">1.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E5%B1%82Embedding"><span class="toc-number">1.1.</span> <span class="toc-text">嵌入层Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Self-Attention%EF%BC%89"><span class="toc-number">1.2.</span> <span class="toc-text">自注意力机制（Self-Attention）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scaled-Dot-Product-Attention%EF%BC%88%E6%A0%B8%E5%BF%83%E8%BF%90%E7%AE%97%EF%BC%89"><span class="toc-number">1.2.1.</span> <span class="toc-text">Scaled Dot-Product Attention（核心运算）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Multi-Head-Attention%EF%BC%88%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%89"><span class="toc-number">1.2.2.</span> <span class="toc-text">Multi-Head Attention（多头注意力）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoder-%E7%BB%93%E6%9E%84"><span class="toc-number">1.3.</span> <span class="toc-text">Encoder 结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Add-Norm%E5%B1%82"><span class="toc-number">1.3.1.</span> <span class="toc-text">Add &amp; Norm层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C%EF%BC%88Feed-Forward-Network-FFN%EF%BC%89"><span class="toc-number">1.3.2.</span> <span class="toc-text">前馈网络（Feed-Forward Network, FFN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder"><span class="toc-number">1.3.3.</span> <span class="toc-text">Encoder</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decoder-%E7%BB%93%E6%9E%84"><span class="toc-number">1.4.</span> <span class="toc-text">Decoder 结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E4%B8%AA-Multi-Head-Attention"><span class="toc-number">1.4.1.</span> <span class="toc-text">第一个 Multi-Head Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E4%B8%AA-Multi-Head-Attention"><span class="toc-number">1.4.2.</span> <span class="toc-text">第二个 Multi-Head Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E7%9A%84%E7%BB%93%E6%9E%84%E5%B7%AE%E5%BC%82"><span class="toc-number">1.5.</span> <span class="toc-text">编码器 &#x2F; 解码器的结构差异</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">1.6.</span> <span class="toc-text">训练</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">&copy;2025 By 沉云</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text">愿你的身旁，永远有幸福的魔法相伴</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  let initFn = window.walineFn || null
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const destroyWaline = ele => ele.destroy()

  const initWaline = (Fn, el = document, path = window.location.pathname) => {
    const waline = Fn({
      el: el.querySelector('#waline-wrap'),
      serverURL: 'https://dream-lime.vercel.app',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      comment: false,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    if (isShuoshuo) {
      window.shuoshuoComment.destroyWaline = () => {
        destroyWaline(waline)
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const loadWaline = (el, path) => {
    if (initFn) initWaline(initFn, el, path)
    else {
      btf.getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css')
        .then(() => import('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js'))
        .then(({ init }) => {
          initFn = init || Waline.init
          initWaline(initFn, el, path)
          window.walineFn = initFn
        })
    }
  }

  if (isShuoshuo) {
    'Waline' === 'Waline'
      ? window.shuoshuoComment = { loadComment: loadWaline } 
      : window.loadOtherComment = loadWaline
    return
  }

  if ('Waline' === 'Waline' || !true) {
    if (true) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
    else setTimeout(loadWaline, 0)
  } else {
    window.loadOtherComment = loadWaline
  }
})()</script></div><div class="aplayer no-destroy" data-id="9430428607" data-server="tencent" data-type="playlist" data-fixed="true" data-autoplay="false"> </div><script defer src="https://cdn.jsdelivr.net/npm/sweetalert2@latest/dist/sweetalert2.all.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script>(() => {
  const destroyAplayer = () => {
    if (window.aplayers) {
      for (let i = 0; i < window.aplayers.length; i++) {
        if (!window.aplayers[i].options.fixed) {
          window.aplayers[i].destroy()
        }
      }
    }
  }

  const runMetingJS = () => {
    typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()
  }

  btf.addGlobalFn('pjaxSend', destroyAplayer, 'destroyAplayer')
  btf.addGlobalFn('pjaxComplete', loadMeting, 'runMetingJS')
})()</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      false 
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>