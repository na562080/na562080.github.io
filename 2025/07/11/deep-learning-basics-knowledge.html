<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>基础知识 | 梦游</title><meta name="author" content="沉云"><meta name="copyright" content="沉云"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习和深度学习一些前提需知">
<meta property="og:type" content="article">
<meta property="og:title" content="基础知识">
<meta property="og:url" content="https://na562080.site/2025/07/11/deep-learning-basics-knowledge.html">
<meta property="og:site_name" content="梦游">
<meta property="og:description" content="机器学习和深度学习一些前提需知">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://na562080.site/img/avatar.webp">
<meta property="article:published_time" content="2025-07-11T02:13:57.000Z">
<meta property="article:modified_time" content="2025-08-05T02:15:32.000Z">
<meta property="article:author" content="沉云">
<meta property="article:tag" content="知识">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://na562080.site/img/avatar.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "基础知识",
  "url": "https://na562080.site/2025/07/11/deep-learning-basics-knowledge.html",
  "image": "https://na562080.site/img/avatar.webp",
  "datePublished": "2025-07-11T02:13:57.000Z",
  "dateModified": "2025-08-05T02:15:32.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "沉云",
      "url": "https://github.com/na562080"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://na562080.site/2025/07/11/deep-learning-basics-knowledge.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"top_n_per_article":3,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '基础知识',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/modify.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="梦游" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(/img/Clifftop_Walk_At_Pourville.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 画廊</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "/img/loading.gif" data-lazy-src="/img/avatar.webp" alt="Logo"><span class="site-name">梦游</span></a><a class="nav-page-title" href="/"><span class="site-name">基础知识</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 画廊</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">基础知识</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-11T02:13:57.000Z" title="发表于 2025-07-11 10:13:57">2025-07-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-05T02:15:32.000Z" title="更新于 2025-08-05 10:15:32">2025-08-05</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%9F%A5%E8%AF%86%E6%9F%A5%E9%98%85/">知识查阅</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p>最底下是参数与变量对应英语查询表，方便快速回忆</p>
</blockquote>
<h1 id="机器学习与深度学习"><a href="#机器学习与深度学习" class="headerlink" title="机器学习与深度学习"></a>机器学习与深度学习</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="泛化能力-Generalization-Ability"><a href="#泛化能力-Generalization-Ability" class="headerlink" title="泛化能力 (Generalization Ability)"></a>泛化能力 (Generalization Ability)</h3><p>模型在训练集外的数据上，保持良好性能(如准确率、误差等)的能力<br>训练误差 是模型在训练数据上的表现，泛化能力是模型对新数据的表现；</p>
<h3 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h3><p>特征空间中模型预测类别发生变化的位置，将不同类别区分开的边界面</p>
<h2 id="数学对象与计算基础"><a href="#数学对象与计算基础" class="headerlink" title="数学对象与计算基础"></a>数学对象与计算基础</h2><h3 id="向量-Vector"><a href="#向量-Vector" class="headerlink" title="向量 (Vector)"></a>向量 (Vector)</h3><p>通常指的是一维数组，也就是固定为一维的</p>
<h3 id="张量-Tensor"><a href="#张量-Tensor" class="headerlink" title="张量 (Tensor)"></a>张量 (Tensor)</h3><p>多维数组的统称，可以看作是标量(0阶张量)、向量(1阶张量)、矩阵(2阶张量)在更高维度上的推广</p>
<h4 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h4><p>PyTorch和NumPy中允许不同形状的张量在运算中自动对齐维度，小张量会被“自动扩展”成跟大张量一样的形状，从而进行逐元素运算，但这个扩展是虚拟的、不会真的复制数据。满足以下两个规则即可触发：</p>
<ol>
<li>两个张量维度相等  </li>
<li>其中一个维度为 1</li>
</ol>
<h4 id="张量创建维度传参方式比较"><a href="#张量创建维度传参方式比较" class="headerlink" title="张量创建维度传参方式比较"></a>张量创建维度传参方式比较</h4><p>两种写法效果完全一致，仅参数形式不同：</p>
<table>
<thead>
<tr>
<th>写法</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>torch.ones(2, 5, 4)</code></td>
<td>直接传入多个维度作为参数</td>
</tr>
<tr>
<td><code>torch.ones((2, 5, 4))</code></td>
<td>将维度打包成元组传入</td>
</tr>
</tbody></table>
<h3 id="矢量化（vectorization）"><a href="#矢量化（vectorization）" class="headerlink" title="矢量化（vectorization）"></a>矢量化（vectorization）</h3><p>将逐元素的循环操作（如 for-loop）转化为整块的向量或矩阵运算，以便利用底层硬件（如 GPU、SIMD）的<strong>并行</strong>计算能力，从而加速程序执行。</p>
<h2 id="模型结构与表达"><a href="#模型结构与表达" class="headerlink" title="模型结构与表达"></a>模型结构与表达</h2><h3 id="仿射变换（Affine-Transformation）"><a href="#仿射变换（Affine-Transformation）" class="headerlink" title="仿射变换（Affine Transformation）"></a>仿射变换（Affine Transformation）</h3><p>仿射变换是一种保留<strong>点、直线和平行关系</strong>的几何变换，对空间中的点进行旋转、缩放、剪切和平移，且保持直线性和平行性，但不一定保持角度或距离。</p>
<p>$$<br>\mathbf{y} &#x3D; A\mathbf{x} + \mathbf{b}<br>$$</p>
<h3 id="估计值"><a href="#估计值" class="headerlink" title="估计值"></a>估计值</h3><p>当我们的输入包含 $p$ 个特征时，我们将预测结果 $\hat{y}$（通常使用“尖角”符号表示 $\hat{}$ 的估计值）写成如下形式：</p>
<p>$$<br>\hat{y} &#x3D; w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_p x_p<br>$$</p>
<p>向量形式：</p>
<p>$$<br>\hat{y} &#x3D; \mathbf{x}^\top \mathbf{w}<br>$$</p>
<ul>
<li>$w_0$：偏置项（bias），有时通过在 $\mathbf{x}$ 开头加一个常数 1 将其统一进向量计算中</li>
</ul>
<h3 id="全连接层（fully-connected-layer）-稠密层（dense-layer）"><a href="#全连接层（fully-connected-layer）-稠密层（dense-layer）" class="headerlink" title="全连接层（fully-connected layer）&#x2F; 稠密层（dense layer）"></a>全连接层（fully-connected layer）&#x2F; 稠密层（dense layer）</h3><p>线性回归中，每个输入都与每个输出相连。</p>
<h3 id="嵌入模型（Embedding）"><a href="#嵌入模型（Embedding）" class="headerlink" title="嵌入模型（Embedding）"></a>嵌入模型（Embedding）</h3><p>指将高维、复杂或非结构化的数据（如文本、图像、语音等）<br>通过某种映射函数，转换成一个低维且稠密的向量表示（通常是实数向量）。  </p>
<p>这种向量表示保留了原始数据的语义或特征信息，使得相似的数据在向量空间中距离较近，<br>从而便于计算机进行相似度比较、聚类、分类和其他机器学习任务。</p>
<h3 id="阈值单元近似（Threshold-Unit-Approximation）"><a href="#阈值单元近似（Threshold-Unit-Approximation）" class="headerlink" title="阈值单元近似（Threshold Unit Approximation）"></a>阈值单元近似（Threshold Unit Approximation）</h3><p>早期神经网络中，阈值单元（Threshold Unit）是一种模拟生物神经元“激发&#x2F;不激发”行为的模型：</p>
<p>$$<br>f(x) &#x3D;<br>\begin{cases}<br>1 &amp; \text{if } x \geq \theta \<br>0 &amp; \text{if } x &lt; \theta<br>\end{cases}<br>$$</p>
<p>其中  $\theta$ 是阈值。<br>该函数是一个阶跃函数（Step Function）连续且不可导，无法用于梯度下降等优化方法。</p>
<h2 id="模型训练与优化"><a href="#模型训练与优化" class="headerlink" title="模型训练与优化"></a>模型训练与优化</h2><h3 id="超参数（Hyperparameter）"><a href="#超参数（Hyperparameter）" class="headerlink" title="超参数（Hyperparameter）"></a>超参数（Hyperparameter）</h3><p><strong>超参数</strong>是用于控制模型结构或训练过程的参数，<strong>不会在训练过程中被模型自动学习</strong>，需要训练开始前由用户设定，或通过搜索算法选定。<br>超参数影响训练过程和模型性能，不参与反向传播。</p>
<h3 id="调参（hyperparameter-tuning）"><a href="#调参（hyperparameter-tuning）" class="headerlink" title="调参（hyperparameter tuning）"></a>调参（hyperparameter tuning）</h3><p>选择和优化超参数的过程，目标是找到一组能使模型在验证集上表现最好的超参数组合。</p>
<h2 id="模型性能与泛化"><a href="#模型性能与泛化" class="headerlink" title="模型性能与泛化"></a>模型性能与泛化</h2><h3 id="训练误差（training-error）"><a href="#训练误差（training-error）" class="headerlink" title="训练误差（training error）"></a>训练误差（training error）</h3><p>模型在训练数据集上计算得到的误差</p>
<h3 id="泛化误差（generalization-error）"><a href="#泛化误差（generalization-error）" class="headerlink" title="泛化误差（generalization error）"></a>泛化误差（generalization error）</h3><p>模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p>
<h3 id="泛化（generalization）"><a href="#泛化（generalization）" class="headerlink" title="泛化（generalization）"></a>泛化（generalization）</h3><p>模型在<strong>未见过的新数据上表现良好</strong>的能力。</p>
<h3 id="过拟合（overfitting）"><a href="#过拟合（overfitting）" class="headerlink" title="过拟合（overfitting）"></a>过拟合（overfitting）</h3><p>模型在训练数据上拟合的比在潜在分布中更接近的现象。</p>
<h3 id="正则化（regularization）"><a href="#正则化（regularization）" class="headerlink" title="正则化（regularization）"></a>正则化（regularization）</h3><p>用于对抗过拟合</p>
<h2 id="特征表示与编码"><a href="#特征表示与编码" class="headerlink" title="特征表示与编码"></a>特征表示与编码</h2><h3 id="独热编码（one-hot-encoding）"><a href="#独热编码（one-hot-encoding）" class="headerlink" title="独热编码（one-hot encoding）"></a>独热编码（one-hot encoding）</h3><p>将类别变量转换为二进制向量的方式。对于有 $N$ 个不同取值的离散特征，每个取值用一个长度为 $N$ 的向量表示，其中只有该取值对应的索引为 1，其余为 0。</p>
<p>消除类别之间的“顺序性”或“大小关系”的假象(如果以索引方式0，1，2…模型会认为是一大类，有着先后顺序，one-hot对象距离都是1)。</p>
<p>线性模型由于把索引当作连续变量计算尤其受影响。</p>
<h2 id="概率与统计基础"><a href="#概率与统计基础" class="headerlink" title="概率与统计基础"></a>概率与统计基础</h2><h3 id="最大似然估计（MLE）"><a href="#最大似然估计（MLE）" class="headerlink" title="最大似然估计（MLE）"></a>最大似然估计（MLE）</h3><p>对于给定的观测数据希望能从所有的参数中找出能最大概率生成观测数据的参数作为估计结果。</p>
<p>以抛硬币为例，假设正面朝上的概率记为 $p$，反面为 $1 - p$，顺序抛出结果是正正反。可以得出$L(p) &#x3D; p^2 (1 - p)$。为了发生概率最大，展开并求导得到 $p &#x3D; \frac{2}{3}$ 时最大。也就是说我们会设定参数为正面$2&#x2F;3$，反面$1&#x2F;3$。</p>
<blockquote>
<p>可能会疑惑这似乎不符合实际概率……这只是基于观测数据的估计，并不一定代表真实的硬币概率。事实上，根据大数定律，最大似然估计的参数会逐渐趋近真实概率，这里只是样本数过少而已。</p>
</blockquote>
<h4 id="概率（Probability）和似然性（Likelihood）的区别"><a href="#概率（Probability）和似然性（Likelihood）的区别" class="headerlink" title="概率（Probability）和似然性（Likelihood）的区别"></a>概率（Probability）和似然性（Likelihood）的区别</h4><table>
<thead>
<tr>
<th>方面</th>
<th>概率（Probability）</th>
<th>似然性（Likelihood）</th>
</tr>
</thead>
<tbody><tr>
<td>定义</td>
<td>给定参数 $\theta$，事件 $X$ 出现的概率</td>
<td>给定数据 $X$，评估参数 $\theta$ 的合理性（似然）</td>
</tr>
<tr>
<td>数学表达</td>
<td>$P(X \mid \theta)$</td>
<td>$L(\theta \mid X) &#x3D; P(X \mid \theta)$（但 $\theta$ 为变量）</td>
</tr>
<tr>
<td>变量角色</td>
<td>$X$ 是变量，$\theta$ 是固定的</td>
<td>$\theta$ 是变量，$X$ 是已知观测值</td>
</tr>
</tbody></table>
<h2 id="语言模型与知识系统"><a href="#语言模型与知识系统" class="headerlink" title="语言模型与知识系统"></a>语言模型与知识系统</h2><h3 id="大语言模型（Large-Language-Model）"><a href="#大语言模型（Large-Language-Model）" class="headerlink" title="大语言模型（Large Language Model）"></a>大语言模型（Large Language Model）</h3><p>LLM模型是一种基于深度学习的大规模预训练语言模型，能够理解和生成自然语言文本。</p>
<h3 id="重排序器（Reranker）"><a href="#重排序器（Reranker）" class="headerlink" title="重排序器（Reranker）"></a>重排序器（Reranker）</h3><p>对初步检索或生成的候选结果进行重新排序，把更相关、更优质的结果排在前面。</p>
<h3 id="知识库-Knowledge-Base，缩写KB"><a href="#知识库-Knowledge-Base，缩写KB" class="headerlink" title="知识库 (Knowledge Base，缩写KB)"></a>知识库 (Knowledge Base，缩写KB)</h3><p>一个系统化存储、管理和检索结构化或半结构化知识的集合。<br>它可以包含事实、规则、概念、实体关系等多种信息形式，旨在支持自动推理、问答和决策等任务。</p>
<h2 id="数据与噪声"><a href="#数据与噪声" class="headerlink" title="数据与噪声"></a>数据与噪声</h2><h3 id="噪声（Noise）"><a href="#噪声（Noise）" class="headerlink" title="噪声（Noise）"></a>噪声（Noise）</h3><p>噪声指数据或信号中随机存在的扰动或误差，通常不包含有用信息。噪声影响模型的训练效果及泛化能力。</p>
<h3 id="白噪声（White-Noise）"><a href="#白噪声（White-Noise）" class="headerlink" title="白噪声（White Noise）"></a>白噪声（White Noise）</h3><p>在时间序列建模（如 AR、MA、ARIMA）中，序列 ({\epsilon_t})（其中 (\epsilon_t) 表示时间 (t) 时刻的噪声值）被称为白噪声需满足以下条件：</p>
<ul>
<li><strong>零均值：</strong></li>
</ul>
<p>$$<br>E[\epsilon_t] &#x3D; 0<br>$$</p>
<ul>
<li><strong>常数且有限的方差：</strong></li>
</ul>
<p>$$<br>\operatorname{Var}(\epsilon_t) &#x3D; \sigma^2 &lt; \infty, \quad \forall t<br>$$</p>
<ul>
<li><strong>无自相关（不同时间点之间不相关）：</strong></li>
</ul>
<p>$$<br>\operatorname{Cov}(\epsilon_t, \epsilon_s) &#x3D; 0, \quad \forall t \neq s<br>$$</p>
<p>这里的“无自相关”意味着不同时间点的噪声值之间不存在<strong>线性相关关系</strong>，但不一定完全独立。</p>
<p>这表明白噪声是一种纯随机过程，不含任何可预测的结构。</p>
<p>“白噪声”名称由来：</p>
<p>类似于光学中的白光包含所有可见光频率，信号处理中的白噪声在频率域上的功率谱密度是均匀分布的，代表所有频率成分权重相等。时间序列中的白噪声对应频谱平坦，意味着没有周期性或结构，完全不可预测。</p>
<p>在时间序列模型中，白噪声通常被用作理想化的随机扰动项，代表纯随机误差。</p>
<h2 id="深度学习核心概念补充"><a href="#深度学习核心概念补充" class="headerlink" title="深度学习核心概念补充"></a>深度学习核心概念补充</h2><h3 id="自相关（Autocorrelation）"><a href="#自相关（Autocorrelation）" class="headerlink" title="自相关（Autocorrelation）"></a>自相关（Autocorrelation）</h3><p>自相关是一个序列与其自身滞后版本之间的相似度度量，用于分析时间序列中是否存在周期性或结构。</p>
<p>$$<br>R(k) &#x3D; \frac{1}{N-k} \sum_{t&#x3D;1}^{N-k} (x_t - \bar{x})(x_{t+k} - \bar{x})<br>$$</p>
<ul>
<li>$k$：滞后阶数  </li>
<li>$\bar{x}$：样本均值  </li>
<li>$N$：样本数量</li>
</ul>
<h3 id="卷积神经网络（Convolutional-Neural-Network-CNN）"><a href="#卷积神经网络（Convolutional-Neural-Network-CNN）" class="headerlink" title="卷积神经网络（Convolutional Neural Network, CNN）"></a>卷积神经网络（Convolutional Neural Network, CNN）</h3><p>CNN 主要用于处理具有网格结构的数据（如图像），通过卷积操作提取局部特征，并利用权值共享降低参数量。</p>
<p>基本操作包括卷积层、激活函数、池化层等。</p>
<ul>
<li><strong>卷积操作</strong>：</li>
</ul>
<p>设输入为二维矩阵 $X$，卷积核为 $K$，输出特征图为 $Y$，则</p>
<p>$$<br>Y(i,j) &#x3D; \sum_m \sum_n X(i+m, j+n) \cdot K(m,n)<br>$$</p>
<ul>
<li><strong>卷积层输出计算公式</strong>：</li>
</ul>
<p>$$<br>y_{i,j,k} &#x3D; \sum_{c&#x3D;1}^{C} \sum_{m&#x3D;1}^{M} \sum_{n&#x3D;1}^{N} x_{i+m-1, j+n-1, c} \cdot w_{m,n,c,k} + b_k<br>$$</p>
<p>其中：</p>
<ul>
<li><p>$x$ 为输入特征图，尺寸为 $H \times W \times C$（高度×宽度×通道数）</p>
</li>
<li><p>$w$ 为卷积核，尺寸为 $M \times N \times C \times K$（卷积核高×卷积核宽×输入通道数×输出通道数）</p>
</li>
<li><p>$b_k$ 为偏置</p>
</li>
<li><p>$y$ 为输出特征图，尺寸为 $(H - M + 1) \times (W - N + 1) \times K$</p>
</li>
<li><p><strong>池化操作</strong>（以最大池化为例）：</p>
</li>
</ul>
<p>$$<br>y_{i,j,k} &#x3D; \max_{m&#x3D;1,…,M, ; n&#x3D;1,…,N} x_{s i + m -1, s j + n -1, k}<br>$$</p>
<p>其中，$s$ 是步长。</p>
<p>CNN 通过堆叠卷积层和池化层，能够自动学习图像中的多层次特征。</p>
<h4 id="卷积（Convolution）"><a href="#卷积（Convolution）" class="headerlink" title="卷积（Convolution）"></a>卷积（Convolution）</h4><p>卷积是一种提取局部区域特征的运算，广泛用于图像、语音、序列等数据中。</p>
<p>一维卷积：</p>
<p>$$<br>s(t) &#x3D; (x * w)(t) &#x3D; \sum_{\tau} x(\tau) w(t - \tau)<br>$$</p>
<p>二维卷积（常用于图像）：</p>
<p>$$<br>S(i, j) &#x3D; \sum_m \sum_n X(i+m, j+n) \cdot W(m, n)<br>$$</p>
<p>特点：</p>
<ul>
<li>局部连接  </li>
<li>参数共享  </li>
<li>可提取边缘、纹理等局部特征</li>
</ul>
<h4 id="池化（Pooling）"><a href="#池化（Pooling）" class="headerlink" title="池化（Pooling）"></a>池化（Pooling）</h4><p>用于降低空间维度、减少计算量、提高平移不变性。</p>
<ul>
<li><strong>最大池化（Max Pooling）：</strong> 取窗口内最大值  </li>
<li><strong>平均池化（Average Pooling）：</strong> 取窗口内平均值</li>
</ul>
<h3 id="马尔可夫性质（Markov-Property）"><a href="#马尔可夫性质（Markov-Property）" class="headerlink" title="马尔可夫性质（Markov Property）"></a>马尔可夫性质（Markov Property）</h3><p>马尔可夫性质：未来状态仅与当前状态有关，与过去无关。</p>
<p>$$<br>P(X_{t+1} \mid X_t, X_{t-1}, \dots, X_0) &#x3D; P(X_{t+1} \mid X_t)<br>$$</p>
<h3 id="循环神经网络（Recurrent-Neural-Network-RNN）"><a href="#循环神经网络（Recurrent-Neural-Network-RNN）" class="headerlink" title="循环神经网络（Recurrent Neural Network, RNN）"></a>循环神经网络（Recurrent Neural Network, RNN）</h3><p>用于处理<strong>序列数据</strong>，具有记忆信息的能力。</p>
<p>每个时刻 $t$ 的状态更新如下：</p>
<p>$$<br>h_t &#x3D; \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)<br>$$</p>
<p>$$<br>y_t &#x3D; W_{hy} h_t + b_y<br>$$</p>
<p>存在梯度消失&#x2F;爆炸问题，难以捕捉长距离依赖。</p>
<h3 id="长短期记忆网络（Long-Short-Term-Memory-LSTM）"><a href="#长短期记忆网络（Long-Short-Term-Memory-LSTM）" class="headerlink" title="长短期记忆网络（Long Short-Term Memory, LSTM）"></a>长短期记忆网络（Long Short-Term Memory, LSTM）</h3><p>通过引入门控机制（遗忘门、输入门、输出门）缓解 RNN 的长依赖问题。</p>
<h3 id="门控循环单元（Gated-Recurrent-Unit-GRU）"><a href="#门控循环单元（Gated-Recurrent-Unit-GRU）" class="headerlink" title="门控循环单元（Gated Recurrent Unit, GRU）"></a>门控循环单元（Gated Recurrent Unit, GRU）</h3><p>GRU 是 LSTM 的简化版本，合并了遗忘门和输入门。</p>
<p>更新公式：</p>
<ul>
<li>更新门：</li>
</ul>
<p>$$<br>z_t &#x3D; \sigma(W_z x_t + U_z h_{t-1})<br>$$</p>
<ul>
<li>重置门：</li>
</ul>
<p>$$<br>r_t &#x3D; \sigma(W_r x_t + U_r h_{t-1})<br>$$</p>
<ul>
<li>候选状态：</li>
</ul>
<p>$$<br>\tilde{h}<em>t &#x3D; \tanh(W_h x_t + U_h (r_t \odot h</em>{t-1}))<br>$$</p>
<ul>
<li>最终状态：</li>
</ul>
<p>$$<br>h_t &#x3D; (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t<br>$$</p>
<h3 id="注意力机制（Attention）"><a href="#注意力机制（Attention）" class="headerlink" title="注意力机制（Attention）"></a>注意力机制（Attention）</h3><p>注意力机制允许模型在处理每个输入位置时<strong>动态关注</strong>不同的上下文部分。</p>
<p>最基础的加性注意力或缩放点积注意力公式如下：</p>
<p>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right) V<br>$$</p>
<ul>
<li>$Q$：Query 查询</li>
<li>$K$：Key 键</li>
<li>$V$：Value 值</li>
<li>$d_k$：Key 的维度（用于缩放）</li>
</ul>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p>Transformer 架构使用<strong>自注意力机制</strong>，完全抛弃了循环结构，广泛用于自然语言处理和生成任务。</p>
<p>基本结构：</p>
<ol>
<li>多头注意力层（Multi-head Attention）</li>
<li>前馈网络（Feed Forward Network）</li>
<li>残差连接（Residual Connection）+ LayerNorm</li>
</ol>
<p>每个子层的形式：</p>
<p>$$<br>\text{LayerNorm}(x + \text{Sublayer}(x))<br>$$</p>
<p>Transformer 的优势：</p>
<ul>
<li>并行训练  </li>
<li>捕捉长距离依赖  </li>
<li>可扩展性强</li>
</ul>
<h3 id="残差连接（Residual-Connection）"><a href="#残差连接（Residual-Connection）" class="headerlink" title="残差连接（Residual Connection）"></a>残差连接（Residual Connection）</h3><p>为解决深层神经网络训练困难，引入“跳跃连接”结构：</p>
<p>$$<br>\mathbf{y} &#x3D; \mathcal{F}(\mathbf{x}) + \mathbf{x}<br>$$</p>
<ul>
<li>$\mathcal{F}(\mathbf{x})$：变换函数（如卷积、注意力等）</li>
<li>$\mathbf{x}$：输入本身</li>
</ul>
<p>避免梯度消失，便于训练深层模型，是 ResNet、Transformer 等模型的关键结构。</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout 是一种正则化技术，训练时以一定概率“丢弃”神经元，防止过拟合。</p>
<p>$$<br>h_i^{\text{drop}} &#x3D;<br>\begin{cases}<br>0 &amp; \text{with probability } p \<br>\frac{h_i}{1-p} &amp; \text{otherwise}<br>\end{cases}<br>$$</p>
<ul>
<li>训练时：随机丢弃</li>
<li>推理时：使用全部神经元，但乘以缩放因子</li>
</ul>
<h3 id="批归一化（Batch-Normalization）"><a href="#批归一化（Batch-Normalization）" class="headerlink" title="批归一化（Batch Normalization）"></a>批归一化（Batch Normalization）</h3><p>用于加速训练、提高稳定性，在每一小批次数据上对特征进行标准化：</p>
<p>$$<br>\hat{x}^{(k)} &#x3D; \frac{x^{(k)} - \mu_B^{(k)}}{\sqrt{\sigma_B^{(k)2} + \epsilon}}<br>$$</p>
<p>再线性变换：</p>
<p>$$<br>y^{(k)} &#x3D; \gamma^{(k)} \hat{x}^{(k)} + \beta^{(k)}<br>$$</p>
<ul>
<li>$\mu_B^{(k)}$：第 $k$ 个特征的批次均值</li>
<li>$\sigma_B^{(k)}$：方差</li>
<li>$\gamma, \beta$：可学习参数</li>
</ul>
<p>作用：</p>
<ul>
<li>减少内部协变量偏移（Internal Covariate Shift）</li>
<li>允许更高学习率</li>
<li>减少对初始化的依赖</li>
</ul>
<h3 id="激活函数（Activation-Function）"><a href="#激活函数（Activation-Function）" class="headerlink" title="激活函数（Activation Function）"></a>激活函数（Activation Function）</h3><p>用于引入非线性，增强模型表达能力。</p>
<ul>
<li>Sigmoid：</li>
</ul>
<p>$$<br>\sigma(x) &#x3D; \frac{1}{1 + e^{-x}}, \quad \sigma’(x) &#x3D; \sigma(x)(1 - \sigma(x))<br>$$</p>
<ul>
<li>Tanh：</li>
</ul>
<p>$$<br>\tanh(x) &#x3D; \frac{e^x - e^{-x}}{e^x + e^{-x}}, \quad \frac{d}{dx} \tanh(x) &#x3D; 1 - \tanh^2(x)<br>$$</p>
<ul>
<li>ReLU：</li>
</ul>
<p>$$<br>f(x) &#x3D; \max(0, x), \quad f’(x) &#x3D;<br>\begin{cases}<br>1 &amp; x &gt; 0 \<br>0 &amp; x \leq 0<br>\end{cases}<br>$$</p>
<ul>
<li>Leaky ReLU：</li>
</ul>
<p>$$<br>f(x) &#x3D; \begin{cases}<br>x &amp; x &gt; 0 \<br>\alpha x &amp; x \leq 0<br>\end{cases}<br>$$</p>
<ul>
<li>Softmax（用于多分类）：</li>
</ul>
<p>$$<br>\sigma(z_i) &#x3D; \frac{e^{z_i}}{\sum_j e^{z_j}}<br>$$</p>
<p>输出为概率分布，常用于输出层。</p>
<h3 id="位置编码（Positional-Encoding）"><a href="#位置编码（Positional-Encoding）" class="headerlink" title="位置编码（Positional Encoding）"></a>位置编码（Positional Encoding）</h3><p>用于为输入序列引入位置信息，使模型具备顺序感。</p>
<p>常用正余弦编码方式：</p>
<p>$$<br>\text{PE}<em>{(pos, 2i)} &#x3D; \sin\left(\frac{pos}{10000^{2i &#x2F; d</em>{model}}} \right)<br>$$</p>
<p>$$<br>\text{PE}<em>{(pos, 2i+1)} &#x3D; \cos\left(\frac{pos}{10000^{2i &#x2F; d</em>{model}}} \right)<br>$$</p>
<ul>
<li>$pos$：位置</li>
<li>$i$：维度编号</li>
<li>$d_{model}$：嵌入维度</li>
</ul>
<h1 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h1><h2 id="一般"><a href="#一般" class="headerlink" title="一般"></a>一般</h2><h3 id="范数-Norm"><a href="#范数-Norm" class="headerlink" title="范数(Norm)"></a>范数(Norm)</h3><p>是数学中用来衡量向量“大小”或“长度”的一个函数。本质上是一种数学映射关系，作为一个函数，输入向量，输出非负实数，表示这个向量的大小或长度。</p>
<h3 id="簇"><a href="#簇" class="headerlink" title="簇"></a>簇</h3><p>指在数据集中，一组彼此相似度较高且相互之间距离较近的数据点集合。换句话说，簇是一类在某种度量标准下被划分到一起的样本点的集合，它们内部的相似性最大，而与其他簇之间的差异最大。</p>
<h3 id="基数（Cardinality）"><a href="#基数（Cardinality）" class="headerlink" title="基数（Cardinality）"></a>基数（Cardinality）</h3><p>指一个集合中元素的数量，$C_k$ 表示第 $k$ 个簇的样本集合，$|C_k|$ 表示集合 $C_k$ 的基数（元素个数）。</p>
<h3 id="平凡解"><a href="#平凡解" class="headerlink" title="平凡解"></a>平凡解</h3><p>指显而易见没有讨论的必要的解，但是为了结果的完整性仍需考虑。<br>一般用不到这个概念。</p>
<h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><h3 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h3><p>标量函数，记作 $\det(A)$ 或 $|A|$，用于判断 $n \times n$ 方阵 $A$ 是否可逆，当且仅当$\det(A) \neq 0$时矩阵可逆。</p>
<h3 id="单位矩阵（Identity-Matrix）"><a href="#单位矩阵（Identity-Matrix）" class="headerlink" title="单位矩阵（Identity Matrix）"></a>单位矩阵（Identity Matrix）</h3><p>主对角线为 1，其余为 0 的方阵，记作 $I_n$，下标 $n$ 代表矩阵阶数。</p>
<h3 id="协方差矩阵（Covariance-Matrix）"><a href="#协方差矩阵（Covariance-Matrix）" class="headerlink" title="协方差矩阵（Covariance Matrix）"></a>协方差矩阵（Covariance Matrix）</h3><p>协方差矩阵用来衡量各个特征之间的<strong>线性相关性</strong>。它是一个 $m \times m$（特征数量）的方阵，矩阵中的每个元素表示第 $i$ 个特征和第 $j$ 个特征之间的协方差。</p>
<p>协方差公式（样本协方差）：<br>$$<br>\text{cov}(X_i, X_j) &#x3D; \frac{1}{n - 1} \sum_{k&#x3D;1}^{n} (X_{ik} - \bar{X}<em>i)(X</em>{jk} - \bar{X}_j)<br>$$</p>
<p>其中：  </p>
<ul>
<li>$X_{ik}$ 表示第 $k$ 个样本在第 $i$ 个特征上的取值。  </li>
<li>$\bar{X}_i$ 表示第 $i$ 个特征的均值。</li>
</ul>
<p>性质：  </p>
<ul>
<li>如果 $\text{cov}(X_i, X_j) &gt; 0$，说明两个特征正相关。  </li>
<li>如果 $\text{cov}(X_i, X_j) &lt; 0$，说明两个特征负相关。  </li>
<li>如果 $\text{cov}(X_i, X_j) \approx 0$，说明两个特征几乎没有<strong>线性相关性</strong>，但可能存在非线性关系。</li>
</ul>
<h4 id="为什么分母是-n-1-：（选看，公式能推但有点费劲）"><a href="#为什么分母是-n-1-：（选看，公式能推但有点费劲）" class="headerlink" title="为什么分母是 $n - 1$：（选看，公式能推但有点费劲）"></a>为什么分母是 $n - 1$：（选看，公式能推但有点费劲）</h4><ul>
<li>当我们用<strong>样本数据</strong>来估计总体的协方差时，使用 $n - 1$ 作为分母可以得到<strong>无偏估计</strong>，让结果更接近真实总体。  </li>
<li>如果使用的是整个总体数据，分母可以用 $n$。</li>
</ul>
<p>“无偏”意味着样本方差的期望值应等于总体方差，即样本方差在长期来看既不会系统性偏大也不会偏小。</p>
<p>样本方差通常用分母 $n - 1$ 而非 $n$，这是因为样本均值是基于同一组样本计算的，与样本数据紧密相关。用样本均值代替总体均值计算偏差时，样本数据的波动被低估，导致样本方差的直接计算值总体偏小。</p>
<p>统计学关注的是大量独立重复抽样的长期性质。假设从总体反复抽取大量样本，计算每个样本的方差，再取这些方差的平均值，发现如果分母用 $n$，则平均样本方差会偏小于真实总体方差。</p>
<p>这是因为样本数据相对于样本均值的波动“向内收缩”，样本均值是样本内数据的固定参考点，而总体均值是全局固定，导致样本方差计算时缺少一个自由度。</p>
<p>采用分母 $n - 1$，即自由度校正，能够消除这种系统性偏差，使样本方差成为总体方差的无偏估计，从而保证样本方差的长期平均值准确反映总体波动。</p>
<h3 id="数学推导"><a href="#数学推导" class="headerlink" title="数学推导"></a>数学推导</h3><p>假设我们有独立同分布的样本：<br>$$<br>X_1, X_2, \ldots, X_n<br>$$<br>总体的真实均值是 $\mu$，但是我们不知道它。样本均值是：<br>$$<br>\bar{X} &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^n X_i<br>$$</p>
<p>样本平方差和：<br>$$<br>S &#x3D; \sum_{i&#x3D;1}^n (X_i - \bar{X})^2<br>$$</p>
<p>为了和总体均值 $\mu$ 联系起来，我们在括号里**加减 $\mu$**，这样写：<br>$$<br>X_i - \bar{X} &#x3D; (X_i - \mu) - (\bar{X} - \mu)<br>$$</p>
<p>把它带回去，展开平方和：<br>$$<br>S &#x3D; \sum_{i&#x3D;1}^n \big[(X_i - \mu) - (\bar{X} - \mu)\big]^2<br>$$</p>
<p>展开平方项得：<br>$$<br>S &#x3D; \sum_{i&#x3D;1}^n (X_i - \mu)^2 - 2(\bar{X} - \mu) \sum_{i&#x3D;1}^n (X_i - \mu) + \sum_{i&#x3D;1}^n (\bar{X} - \mu)^2<br>$$</p>
<p>因为 $\bar{X} - \mu$ 是一个常数，不随 $i$ 变化，且有 $n$ 个项，所以：<br>$$<br>S &#x3D; \sum_{i&#x3D;1}^n (X_i - \mu)^2 - 2(\bar{X} - \mu) \sum_{i&#x3D;1}^n (X_i - \mu) + n(\bar{X} - \mu)^2<br>$$</p>
<p>接着，注意到：<br>$$<br>\sum_{i&#x3D;1}^n (X_i - \mu) &#x3D; n(\bar{X} - \mu)<br>$$</p>
<p>带入上式，得到：<br>$$<br>S &#x3D; \sum_{i&#x3D;1}^n (X_i - \mu)^2 - 2n(\bar{X} - \mu)^2 + n(\bar{X} - \mu)^2 &#x3D; \sum_{i&#x3D;1}^n (X_i - \mu)^2 - n(\bar{X} - \mu)^2<br>$$</p>
<p>对上式两边取期望：<br>$$<br>\mathbb{E}[S] &#x3D; \mathbb{E}\left[\sum_{i&#x3D;1}^n (X_i - \mu)^2\right] - \mathbb{E}\left[n(\bar{X} - \mu)^2\right]<br>$$</p>
<p>由于每个 $X_i$ 独立同分布且方差是 $\sigma^2$，所以：<br>$$<br>\mathbb{E}[(X_i - \mu)^2] &#x3D; \sigma^2<br>\Rightarrow \mathbb{E}\left[\sum_{i&#x3D;1}^n (X_i - \mu)^2\right] &#x3D; n \sigma^2<br>$$</p>
<p>样本均值的方差是：<br>$$<br>\operatorname{Var}(\bar{X}) &#x3D; \frac{\sigma^2}{n} \Rightarrow \mathbb{E}[(\bar{X} - \mu)^2] &#x3D; \frac{\sigma^2}{n}<br>$$</p>
<p>所以：<br>$$<br>\mathbb{E}[n(\bar{X} - \mu)^2] &#x3D; n \times \frac{\sigma^2}{n} &#x3D; \sigma^2<br>$$</p>
<p>因此，<br>$$<br>\mathbb{E}[S] &#x3D; n \sigma^2 - \sigma^2 &#x3D; (n-1) \sigma^2<br>$$</p>
<p>这就是说，用样本均值计算的平方差和的期望是 $(n-1)\sigma^2$，而不是 $n \sigma^2$。</p>
<p>这就是为什么样本方差的分母不能用 $n$，而是用 $n-1$ — 这样才能保证样本方差的期望等于总体方差，使其成为无偏估计。</p>
<p>换句话说：  </p>
<ul>
<li><strong>样本均值是基于样本数据计算的，会导致方差计算的波动被低估。</strong>  </li>
<li><strong>用 $n-1$ 修正分母，补偿这个系统性偏差，得到更准确的方差估计。</strong></li>
</ul>
<p>也就是对所有可能的值 $x$ 按概率密度加权求积分，得到平均值。</p>
<h3 id="正半定矩阵（Positive-Semidefinite-Matrix）"><a href="#正半定矩阵（Positive-Semidefinite-Matrix）" class="headerlink" title="正半定矩阵（Positive Semidefinite Matrix）"></a>正半定矩阵（Positive Semidefinite Matrix）</h3><p>“正半定”是“正半定矩阵”的简称，满足下面条件</p>
<ul>
<li>$A$ 是对称矩阵（即 $A &#x3D; A^T$，对称方阵），</li>
<li>对任意非零向量 $x$，都有 $x^T A x \geq 0$。</li>
</ul>
<p>这表示矩阵 $A$ 作用于任意向量时，不会让“二次型”变成负值，只可能是零或者正数。</p>
<ul>
<li><p><strong>正定矩阵</strong>：对所有非零向量 $x$，都有</p>
<p>$$<br>x^T A x &gt; 0<br>$$</p>
</li>
<li><p><strong>正半定矩阵</strong>：对所有向量 $x$，都有</p>
<p>$$<br>x^T A x \geq 0<br>$$</p>
</li>
</ul>
<p>（允许等于零）</p>
<h2 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h2><h3 id="基本概率论"><a href="#基本概率论" class="headerlink" title="基本概率论"></a>基本概率论</h3><h4 id="大数定律（Law-of-Large-Numbers-LLN）"><a href="#大数定律（Law-of-Large-Numbers-LLN）" class="headerlink" title="大数定律（Law of Large Numbers, LLN）"></a>大数定律（Law of Large Numbers, LLN）</h4><p>随机变量的平均值在大量重复试验后趋近于期望值</p>
<h4 id="数学表述（弱大数定律）"><a href="#数学表述（弱大数定律）" class="headerlink" title="数学表述（弱大数定律）"></a>数学表述（弱大数定律）</h4><p>$$<br>\lim_{n \to \infty} \frac{1}{n} \sum_{i&#x3D;1}^n X_i &#x3D; \mu \quad \text{（几乎必然收敛）}<br>$$</p>
<p>这意味着，当样本数量 $n$ 趋于无穷大时，样本平均值几乎必然会收敛到真实的期望值 $\mu$。也就是理论均值。</p>
<h3 id="抽样（sampling）"><a href="#抽样（sampling）" class="headerlink" title="抽样（sampling）"></a>抽样（sampling）</h3><p>从概率分布中抽取样本的过程</p>
<h3 id="数学期望（Expectation，-E-）"><a href="#数学期望（Expectation，-E-）" class="headerlink" title="数学期望（Expectation，$E$）"></a>数学期望（Expectation，$E$）</h3><p>理论观测样本的均值，数学期望 $E[X]$ 表示对随机变量 $X$ 的期望值，表示随机变量在长期重复试验中的平均结果。</p>
<ul>
<li><p>离散型：<br>$$<br>E[X] &#x3D; \sum_i x_i p_i<br>$$</p>
<p>数学性质：<br>$$<br>\mathbb{E}[AX + b] &#x3D; A , \mathbb{E}[X] + b<br>$$</p>
</li>
<li><p>连续型：<br>$$<br>E[X] &#x3D; \int_{-\infty}^{+\infty} x f(x) , dx<br>$$</p>
</li>
</ul>
<h3 id="均值（Sample-Mean）"><a href="#均值（Sample-Mean）" class="headerlink" title="均值（Sample Mean）"></a>均值（Sample Mean）</h3><p>实际观测样本的均值，是具体样本数据的平均数，会随着样本改变而波动。</p>
<h4 id="分布（distribution）"><a href="#分布（distribution）" class="headerlink" title="分布（distribution）"></a>分布（distribution）</h4><p>事件的概率分配</p>
<h4 id="多项分布（multinomial-distribution）"><a href="#多项分布（multinomial-distribution）" class="headerlink" title="多项分布（multinomial distribution）"></a>多项分布（multinomial distribution）</h4><p>将概率分配给一些离散选择的分布</p>
<h4 id="Var（Variance，方差）"><a href="#Var（Variance，方差）" class="headerlink" title="Var（Variance，方差）"></a>Var（Variance，方差）</h4><p>数据偏离期望的程度<br>$$<br>\text{Var}(X) &#x3D; E\left[(X - \mu)^2\right]<br>$$</p>
<h4 id="样本空间（sample-space）-结果空间（outcome-space）"><a href="#样本空间（sample-space）-结果空间（outcome-space）" class="headerlink" title="样本空间（sample space）&#x2F; 结果空间（outcome space）"></a>样本空间（sample space）&#x2F; 结果空间（outcome space）</h4><p>表示为一个集合 $S$，包含所有可能结果</p>
<h4 id="事件（event）"><a href="#事件（event）" class="headerlink" title="事件（event）"></a>事件（event）</h4><p>一组给定样本空间的随机结果，记作 $A \subseteq S$</p>
<h4 id="概率（probability）"><a href="#概率（probability）" class="headerlink" title="概率（probability）"></a>概率（probability）</h4><p>将集合映射到真实值的函数，满足以下属性</p>
<ul>
<li><p>对于任意事件，其概率从不会是负数，$P(A) \geq 0$</p>
</li>
<li><p>整个样本空间的概率为1，$P(S) &#x3D; 1$</p>
</li>
<li><p>对于互斥（mutually exclusive）事件，序列中任意一个事件发生的概率等于它们各自发生的概率之和，若 $A_1, A_2, \dots$ 互不相交，则  </p>
<p>$$<br>P\left( \bigcup_{i&#x3D;1}^{\infty} A_i \right) &#x3D; \sum_{i&#x3D;1}^{\infty} P(A_i)<br>$$</p>
</li>
</ul>
<h4 id="先验概率"><a href="#先验概率" class="headerlink" title="先验概率"></a>先验概率</h4><p>先验概率是基于背景常识或者历史数据的统计得出的预判概率，一般只包含一个变量，例如P(A)，P(B)。</p>
<h4 id="联合概率（joint-probability）"><a href="#联合概率（joint-probability）" class="headerlink" title="联合概率（joint probability）"></a>联合概率（joint probability）</h4><p>事件同时发生的概率，两个事件同时发生的概率：$P(A,B)$</p>
<h4 id="条件概率（conditional-probability）"><a href="#条件概率（conditional-probability）" class="headerlink" title="条件概率（conditional probability）"></a>条件概率（conditional probability）</h4><p>一个事件发生后另一个事件发生的概率，一般情况下 $B$ 表示某一个因素，$A$ 表示结果，$P(A|B)$ 表示在因素 $B$ 的条件下 $A$ 发生的概率，即由因求果。<br>条件概率表示在事件 $B$ 已经发生的前提下，事件 $A$ 发生的概率：</p>
<p>$$<br>P(A \mid B) &#x3D; \frac{P(A \cap B)}{P(B)} \quad \text{（前提是 } P(B) &gt; 0 \text{）}<br>$$</p>
<p>其中：  </p>
<ul>
<li>$P(A \cap B)$ 表示事件 $A$ 与事件 $B$ 同时发生的概率。  </li>
<li>$P(B)$ 是事件 $B$ 的概率。</li>
</ul>
<h4 id="后验概率"><a href="#后验概率" class="headerlink" title="后验概率"></a>后验概率</h4><p>由果求因，也就是在知道结果的情况下求原因的概率，例如Y事件是X引起的，那么P(X|Y)就是后验概率，也可以说它是事件发生后的反向条件概率。<br>实际上后验概率就是条件概率，只不过条件概率是由因求果，而后验概率是由果求因。</p>
<h4 id="贝叶斯定理（Bayes’-theorem）"><a href="#贝叶斯定理（Bayes’-theorem）" class="headerlink" title="贝叶斯定理（Bayes’ theorem）"></a>贝叶斯定理（Bayes’ theorem）</h4><p>利用条件概率进行反向推断：<br>$$<br>P(A \mid B) &#x3D; \frac{P(B \mid A) \cdot P(A)}{P(B)} \quad \text{（前提是 } P(B) &gt; 0 \text{）}<br>$$</p>
<h4 id="边际概率（marginal-probability）-边际分布（marginal-distribution）"><a href="#边际概率（marginal-probability）-边际分布（marginal-distribution）" class="headerlink" title="边际概率（marginal probability） &#x2F; 边际分布（marginal distribution）"></a>边际概率（marginal probability） &#x2F; 边际分布（marginal distribution）</h4><p>事件概率求和将所有选择的联合概率聚合在一起</p>
<ul>
<li><p>离散型：</p>
<p>$$<br>P(X &#x3D; x) &#x3D; \sum_y P(X &#x3D; x, Y &#x3D; y)<br>$$</p>
</li>
<li><p>连续型：</p>
<p>$$<br>f_X(x) &#x3D; \int_{-\infty}^{+\infty} f_{X,Y}(x, y) , dy<br>$$</p>
</li>
</ul>
<h1 id="信息论（Information-Theory）"><a href="#信息论（Information-Theory）" class="headerlink" title="信息论（Information Theory）"></a>信息论（Information Theory）</h1><p>涉及编码、解码、传输以及尽可能简洁地处理信息或数据。</p>
<h2 id="信息论基本定理"><a href="#信息论基本定理" class="headerlink" title="信息论基本定理"></a>信息论基本定理</h2><h3 id="香农第一定理（无失真编码定理）"><a href="#香农第一定理（无失真编码定理）" class="headerlink" title="香农第一定理（无失真编码定理）"></a>香农第一定理（无失真编码定理）</h3><p>给定信息源的熵为 $H(X)$，对该信息源的无失真编码，平均码长 $L$ 满足：</p>
<p>$$<br>H(X) \leq L &lt; H(X) + 1<br>$$</p>
<p>（在码长为整数的限制下成立）</p>
<h3 id="香农第二定理（信道编码定理）"><a href="#香农第二定理（信道编码定理）" class="headerlink" title="香农第二定理（信道编码定理）"></a>香农第二定理（信道编码定理）</h3><p>对于容量为 $C$ 的信道，如果传输速率 $R &lt; C$，则存在编码方案使误码率趋近于 0；反之，若 $R &gt; C$，误码率无法降低。</p>
<h3 id="熵的定义（entropy）"><a href="#熵的定义（entropy）" class="headerlink" title="熵的定义（entropy）"></a>熵的定义（entropy）</h3><p>熵度量所有可能事件的信息量的期望值，衡量事件本身信息的不确定性。</p>
<blockquote>
<p>事件空间中所有事件发生的概率，乘以该事件的信息量，即可得到熵</p>
</blockquote>
<p>$$<br>H(X) &#x3D; - \sum_{i&#x3D;1}^n p_i \log p_i<br>$$</p>
<p>一般单位是比特（bit）底数为 2。<br>熵达到最大值时，概率分布为均匀分布。</p>
<p>对于一个必然事件而言，其事件发生的信息熵为0。也就是说，我们不需要传递任何信号（0比特），对方也能知道该事件发生了。</p>
<h3 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h3><p>两个随机变量 $X$ 和 $Y$ 的互信息定义为：</p>
<p>$$<br>I(X; Y) &#x3D; H(X) - H(X|Y) &#x3D; H(Y) - H(Y|X)<br>$$</p>
<p>表示在给定 $Y$ 的条件下，随机变量 $X$ 的不确定性减少的量，即 $X$ 和 $Y$ 之间共享的信息量。</p>
<blockquote>
<p>差值表示共享量？信息共享是一种概率结构中的重合关系，本身没有行为，它是客观存在于变量之间的信息重合度量。<br>可以看看下面的解释<br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/24059517/answer/26750918">如何理解互信息公式的含义?</a></p>
</blockquote>
<p>互信息的本质就是对称的。</p>
<h2 id="信息论基础"><a href="#信息论基础" class="headerlink" title="信息论基础"></a>信息论基础</h2><h3 id="自信息（Information-Content-Surprisal）"><a href="#自信息（Information-Content-Surprisal）" class="headerlink" title="自信息（Information Content &#x2F; Surprisal）"></a>自信息（Information Content &#x2F; Surprisal）</h3><p>对于某一具体事件 $x$，其发生的概率为 $p(x)$，定义该事件的信息量为：</p>
<p>$$<br>I(x) &#x3D; -\log p(x)<br>$$</p>
<p>当事件越罕见（$p(x)$ 越小），信息量 $I(x)$ 越大，表示惊异程度更高。</p>
<h3 id="交叉熵（Cross-Entropy）"><a href="#交叉熵（Cross-Entropy）" class="headerlink" title="交叉熵（Cross-Entropy）"></a>交叉熵（Cross-Entropy）</h3><p>假设真实概率分布为 $P &#x3D; {p_i}$，观察者主观概率分布为 $Q &#x3D; {q_i}$</p>
<blockquote>
<p>$q_i$ 小于 1 则 log 后为负要加负号，H为0表示模型预测完全正确，无穷大时完全错误<br>若为独热标签P，其中对应真实类值为 1，其他为 0，即求和时P可以等价为标量1来计算</p>
</blockquote>
<p>交叉熵用一个预测分布去描述真实分布时，整体的“惊异度”或“编码代价”。衡量预测模型对真实事件编码效率的损失。和信息量本身没有直接一一对应关系</p>
<p>交叉熵定义为：</p>
<p>$$<br>H(P, Q) &#x3D; - \sum_{i} p_i \log q_i<br>$$</p>
<p>交叉熵表示当数据真实分布为 $P$，但使用概率分布 $Q$ 来编码时的平均信息量（即平均惊异程度）。<br>当且仅当 $Q &#x3D; P$ 时，交叉熵达到最小值：</p>
<p>$$<br>H(P, Q)_{\min} &#x3D; H(P) &#x3D; - \sum_i p_i \log p_i<br>$$</p>
<p>交叉熵总是大于等于熵，差值是 KL 散度：</p>
<p>$$<br>D_{KL}(P | Q) &#x3D; H(P, Q) - H(P) \geq 0<br>$$</p>
<blockquote>
<p>log 函数让损失函数对预测错误变得非常敏感<br>log 函数的梯度形式适合和 softmax 输出配合，数学上简洁</p>
</blockquote>
<h1 id="常用术语与定义的英文"><a href="#常用术语与定义的英文" class="headerlink" title="常用术语与定义的英文"></a>常用术语与定义的英文</h1><table>
<thead>
<tr>
<th>中文术语</th>
<th>英文术语</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>参数</td>
<td>Parameter</td>
<td>模型内部需学习的变量（如权重weights、偏置bias）</td>
</tr>
<tr>
<td>权重</td>
<td>Weight</td>
<td>模型连接强度，学习的核心变量</td>
</tr>
<tr>
<td>偏置</td>
<td>Bias</td>
<td>模型的偏移量，辅助调整输出</td>
</tr>
<tr>
<td>超参数</td>
<td>Hyperparameter</td>
<td>训练前设置，不参与梯度更新的控制变量</td>
</tr>
<tr>
<td>学习率</td>
<td>Learning Rate</td>
<td>控制参数更新步长</td>
</tr>
<tr>
<td>优化器</td>
<td>Optimizer</td>
<td>用于训练时更新参数的算法</td>
</tr>
<tr>
<td>损失函数</td>
<td>Loss Function</td>
<td>衡量模型预测误差的函数</td>
</tr>
<tr>
<td>目标函数</td>
<td>Objective Function</td>
<td>优化的函数，通常指损失函数或其变体</td>
</tr>
<tr>
<td>正则化</td>
<td>Regularization</td>
<td>控制模型复杂度，防止过拟合</td>
</tr>
<tr>
<td>L1正则化</td>
<td>L1 Regularization</td>
<td>参数绝对值和作为正则项，促使参数稀疏</td>
</tr>
<tr>
<td>L2正则化</td>
<td>L2 Regularization (Ridge)</td>
<td>参数平方和作为正则项，促使参数小而平滑</td>
</tr>
<tr>
<td>权重衰减</td>
<td>Weight Decay</td>
<td>L2正则化的一种实现方式</td>
</tr>
<tr>
<td>批量大小</td>
<td>Batch Size</td>
<td>每次训练用的样本数量</td>
</tr>
<tr>
<td>迭代次数</td>
<td>Epoch</td>
<td>遍历训练集一次</td>
</tr>
<tr>
<td>梯度</td>
<td>Gradient</td>
<td>损失对参数的偏导数，用于参数更新</td>
</tr>
<tr>
<td>梯度消失</td>
<td>Vanishing Gradient</td>
<td>深层网络训练中梯度变得极小，导致更新缓慢或停滞</td>
</tr>
<tr>
<td>梯度爆炸</td>
<td>Exploding Gradient</td>
<td>梯度过大导致训练不稳定</td>
</tr>
<tr>
<td>激活函数</td>
<td>Activation Function</td>
<td>非线性函数，如ReLU、Sigmoid、Tanh</td>
</tr>
<tr>
<td>激活值</td>
<td>Activation Value</td>
<td>激活函数输出的值</td>
</tr>
<tr>
<td>过拟合</td>
<td>Overfitting</td>
<td>模型在训练集表现很好但泛化能力差</td>
</tr>
<tr>
<td>欠拟合</td>
<td>Underfitting</td>
<td>模型复杂度不足，无法捕捉数据规律</td>
</tr>
<tr>
<td>训练集</td>
<td>Training Set</td>
<td>用于训练模型的数据集</td>
</tr>
<tr>
<td>验证集</td>
<td>Validation Set</td>
<td>用于调参和选择模型的数据集</td>
</tr>
<tr>
<td>测试集</td>
<td>Test Set</td>
<td>用于最终评估模型泛化能力的数据集</td>
</tr>
<tr>
<td>预测</td>
<td>Prediction</td>
<td>模型对新数据的输出结果</td>
</tr>
<tr>
<td>精度</td>
<td>Accuracy</td>
<td>正确预测数占总预测数比例</td>
</tr>
<tr>
<td>召回率</td>
<td>Recall</td>
<td>真阳性数占所有实际正例比例</td>
</tr>
<tr>
<td>精确率</td>
<td>Precision</td>
<td>真阳性数占所有预测为正例比例</td>
</tr>
<tr>
<td>F1分数</td>
<td>F1 Score</td>
<td>精确率和召回率的调和平均值</td>
</tr>
<tr>
<td>ROC曲线</td>
<td>ROC Curve</td>
<td>受试者工作特征曲线，评价分类模型性能</td>
</tr>
<tr>
<td>AUC</td>
<td>Area Under Curve</td>
<td>ROC曲线下面积，模型性能指标</td>
</tr>
<tr>
<td>交叉熵损失</td>
<td>Cross-Entropy Loss</td>
<td>分类任务常用损失函数</td>
</tr>
<tr>
<td>均方误差</td>
<td>Mean Squared Error (MSE)</td>
<td>回归任务常用损失函数</td>
</tr>
<tr>
<td>平均绝对误差</td>
<td>Mean Absolute Error (MAE)</td>
<td>回归任务误差绝对值的平均</td>
</tr>
<tr>
<td>优化算法</td>
<td>Optimization Algorithm</td>
<td>如SGD、Adam等用于训练的算法</td>
</tr>
<tr>
<td>随机梯度下降</td>
<td>Stochastic Gradient Descent (SGD)</td>
<td>每次用小批量数据计算梯度的优化方法</td>
</tr>
<tr>
<td>动量</td>
<td>Momentum</td>
<td>优化算法中用于加速收敛的技术</td>
</tr>
<tr>
<td>学习率衰减</td>
<td>Learning Rate Decay</td>
<td>训练过程中逐步降低学习率</td>
</tr>
<tr>
<td>学习率调度器</td>
<td>Learning Rate Scheduler</td>
<td>按规则动态调整学习率</td>
</tr>
<tr>
<td>提前停止</td>
<td>Early Stopping</td>
<td>根据验证性能提前终止训练防止过拟合</td>
</tr>
<tr>
<td>批归一化</td>
<td>Batch Normalization</td>
<td>归一化层，稳定训练，加速收敛</td>
</tr>
<tr>
<td>Dropout</td>
<td>Dropout</td>
<td>防止过拟合的随机神经元失活技术</td>
</tr>
<tr>
<td>权重初始化</td>
<td>Weight Initialization</td>
<td>参数初始化方法，如Xavier初始化，He初始化</td>
</tr>
<tr>
<td>权重共享</td>
<td>Weight Sharing</td>
<td>不同位置共享同一组权重，节省参数</td>
</tr>
<tr>
<td>参数共享</td>
<td>Parameter Sharing</td>
<td>不同层或位置共享相同参数</td>
</tr>
<tr>
<td>参数冻结</td>
<td>Parameter Freezing</td>
<td>在迁移学习中冻结部分参数不参与训练</td>
</tr>
<tr>
<td>梯度裁剪</td>
<td>Gradient Clipping</td>
<td>限制梯度大小防止梯度爆炸</td>
</tr>
<tr>
<td>梯度累积</td>
<td>Gradient Accumulation</td>
<td>分批计算梯度，累积后一次更新</td>
</tr>
<tr>
<td>多层感知机</td>
<td>Multilayer Perceptron (MLP)</td>
<td>基础前馈神经网络，由输入层、一个或多个隐藏层及输出层组成。每层节点与下一层全连接，隐藏层配非线性激活函数（如ReLU、Sigmoid），能拟合非线性函数。用于分类和回归任务，训练采用反向传播和梯度下降。对高维结构化数据有效，但对空间或序列信息建模有限。</td>
</tr>
<tr>
<td>前馈神经网络</td>
<td>Feedforward Neural Network</td>
<td>无环路连接的神经网络，信息单向传播</td>
</tr>
<tr>
<td>卷积神经网络</td>
<td>Convolutional Neural Network (CNN)</td>
<td>专门处理网格数据（如图像），通过卷积层提取局部空间特征，利用权重共享和池化层减少参数和降维，广泛用于图像、语音等领域。</td>
</tr>
<tr>
<td>卷积层</td>
<td>Convolutional Layer</td>
<td>CNN中用于提取空间特征的层</td>
</tr>
<tr>
<td>池化层</td>
<td>Pooling Layer</td>
<td>降维层，常用最大池化和平均池化</td>
</tr>
<tr>
<td>循环神经网络</td>
<td>Recurrent Neural Network (RNN)</td>
<td>处理序列数据的网络结构，具有循环连接</td>
</tr>
<tr>
<td>长短时记忆网络</td>
<td>Long Short-Term Memory (LSTM)</td>
<td>解决RNN梯度消失问题的变体</td>
</tr>
<tr>
<td>门控循环单元</td>
<td>Gated Recurrent Unit (GRU)</td>
<td>LSTM的简化版，计算效率更高</td>
</tr>
<tr>
<td>注意力机制</td>
<td>Attention Mechanism</td>
<td>聚焦输入关键部分的机制</td>
</tr>
<tr>
<td>自注意力</td>
<td>Self-Attention</td>
<td>输入自身的注意力机制</td>
</tr>
<tr>
<td>Transformer</td>
<td>Transformer</td>
<td>基于自注意力的主流网络架构</td>
</tr>
<tr>
<td>损失梯度</td>
<td>Loss Gradient</td>
<td>损失函数对参数的导数</td>
</tr>
<tr>
<td>反向传播</td>
<td>Backpropagation</td>
<td>计算梯度并更新参数的算法</td>
</tr>
<tr>
<td>模型容量</td>
<td>Model Capacity</td>
<td>模型表达复杂函数的能力</td>
</tr>
<tr>
<td>训练误差</td>
<td>Training Error</td>
<td>模型在训练集上的误差</td>
</tr>
<tr>
<td>验证误差</td>
<td>Validation Error</td>
<td>模型在验证集上的误差</td>
</tr>
<tr>
<td>泛化误差</td>
<td>Generalization Error</td>
<td>模型在未见过数据上的误差</td>
</tr>
<tr>
<td>数据增强</td>
<td>Data Augmentation</td>
<td>通过变换增加训练数据多样性</td>
</tr>
<tr>
<td>迁移学习</td>
<td>Transfer Learning</td>
<td>利用预训练模型知识加速新任务训练</td>
</tr>
<tr>
<td>微调</td>
<td>Fine-tuning</td>
<td>基于预训练模型进行少量参数调整</td>
</tr>
<tr>
<td>批处理</td>
<td>Batch Processing</td>
<td>一次处理多个样本</td>
</tr>
<tr>
<td>小批量梯度下降</td>
<td>Mini-batch Gradient Descent</td>
<td>每次用小批量数据计算梯度更新</td>
</tr>
<tr>
<td>损失曲线</td>
<td>Loss Curve</td>
<td>训练过程中损失值随迭代变化的曲线</td>
</tr>
<tr>
<td>混淆矩阵</td>
<td>Confusion Matrix</td>
<td>分类模型预测结果统计表</td>
</tr>
<tr>
<td>正类</td>
<td>Positive Class</td>
<td>分类中的目标类别</td>
</tr>
<tr>
<td>负类</td>
<td>Negative Class</td>
<td>分类中的非目标类别</td>
</tr>
<tr>
<td>序列模型</td>
<td>Sequence Model</td>
<td>处理序列数据的模型，如RNN、LSTM</td>
</tr>
<tr>
<td>激活分布</td>
<td>Activation Distribution</td>
<td>神经元激活值的统计分布</td>
</tr>
<tr>
<td>线性层</td>
<td>Linear Layer</td>
<td>计算线性变换的神经网络层</td>
</tr>
<tr>
<td>损失函数平滑</td>
<td>Label Smoothing</td>
<td>防止过拟合的标签平滑技巧</td>
</tr>
<tr>
<td>模型剪枝</td>
<td>Model Pruning</td>
<td>减少模型参数以提升推理效率</td>
</tr>
<tr>
<td>量化</td>
<td>Quantization</td>
<td>将模型参数减少位宽以降低存储和计算成本</td>
</tr>
<tr>
<td>蒸馏</td>
<td>Knowledge Distillation</td>
<td>通过“教师-学生”模型传递知识</td>
</tr>
<tr>
<td>生成模型</td>
<td>Generative Model</td>
<td>用于生成数据的模型，如GAN、VAE</td>
</tr>
<tr>
<td>判别模型</td>
<td>Discriminative Model</td>
<td>用于分类或预测的模型</td>
</tr>
<tr>
<td>损失平滑</td>
<td>Loss Smoothing</td>
<td>通过平滑技术减少损失震荡</td>
</tr>
</tbody></table>
<h1 id="代码中常见参数名及解释（英文）"><a href="#代码中常见参数名及解释（英文）" class="headerlink" title="代码中常见参数名及解释（英文）"></a>代码中常见参数名及解释（英文）</h1><table>
<thead>
<tr>
<th>变量名</th>
<th>英文全称</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>weights</td>
<td>Weights</td>
<td>模型的权重参数</td>
</tr>
<tr>
<td>bias</td>
<td>Bias</td>
<td>偏置项</td>
</tr>
<tr>
<td>X</td>
<td>Input &#x2F; Feature Matrix</td>
<td>输入特征矩阵</td>
</tr>
<tr>
<td>y</td>
<td>Output &#x2F; Target</td>
<td>目标变量</td>
</tr>
<tr>
<td>loss</td>
<td>Loss</td>
<td>损失函数</td>
</tr>
<tr>
<td>grad</td>
<td>Gradient</td>
<td>损失函数对参数的梯度</td>
</tr>
<tr>
<td>lr</td>
<td>Learning Rate</td>
<td>学习率</td>
</tr>
<tr>
<td>epoch</td>
<td>Epoch</td>
<td>训练轮数</td>
</tr>
<tr>
<td>batch_size</td>
<td>Batch Size</td>
<td>批大小</td>
</tr>
<tr>
<td>pred</td>
<td>Prediction</td>
<td>预测值</td>
</tr>
<tr>
<td>epsilon</td>
<td>Epsilon</td>
<td>极小数，防止数值不稳定</td>
</tr>
<tr>
<td>mu</td>
<td>Mean</td>
<td>均值</td>
</tr>
<tr>
<td>sigma</td>
<td>Standard Deviation</td>
<td>标准差</td>
</tr>
<tr>
<td>coeff</td>
<td>Coefficient</td>
<td>系数（如高斯函数中的归一化常数）</td>
</tr>
<tr>
<td>exponent</td>
<td>Exponent</td>
<td>指数部分（如高斯函数中的指数表达式）</td>
</tr>
<tr>
<td>theta</td>
<td>Parameter Vector</td>
<td>参数向量（权重的另一种常见符号）</td>
</tr>
<tr>
<td>alpha</td>
<td>Coefficient &#x2F; LR</td>
<td>学习率或正则化参数</td>
</tr>
<tr>
<td>beta</td>
<td>Coefficient</td>
<td>系数，也常用于正则化参数</td>
</tr>
</tbody></table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/na562080">沉云</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://na562080.site/2025/07/11/deep-learning-basics-knowledge.html">https://na562080.site/2025/07/11/deep-learning-basics-knowledge.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://na562080.site" target="_blank">梦游</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%9F%A5%E8%AF%86/">知识</a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">机器学习与深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B-Generalization-Ability"><span class="toc-number">1.1.1.</span> <span class="toc-text">泛化能力 (Generalization Ability)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="toc-number">1.1.2.</span> <span class="toc-text">决策边界</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%AF%B9%E8%B1%A1%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9F%BA%E7%A1%80"><span class="toc-number">1.2.</span> <span class="toc-text">数学对象与计算基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F-Vector"><span class="toc-number">1.2.1.</span> <span class="toc-text">向量 (Vector)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F-Tensor"><span class="toc-number">1.2.2.</span> <span class="toc-text">张量 (Tensor)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">广播机制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%88%9B%E5%BB%BA%E7%BB%B4%E5%BA%A6%E4%BC%A0%E5%8F%82%E6%96%B9%E5%BC%8F%E6%AF%94%E8%BE%83"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">张量创建维度传参方式比较</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A2%E9%87%8F%E5%8C%96%EF%BC%88vectorization%EF%BC%89"><span class="toc-number">1.2.3.</span> <span class="toc-text">矢量化（vectorization）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E4%B8%8E%E8%A1%A8%E8%BE%BE"><span class="toc-number">1.3.</span> <span class="toc-text">模型结构与表达</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%EF%BC%88Affine-Transformation%EF%BC%89"><span class="toc-number">1.3.1.</span> <span class="toc-text">仿射变换（Affine Transformation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%B0%E8%AE%A1%E5%80%BC"><span class="toc-number">1.3.2.</span> <span class="toc-text">估计值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%EF%BC%88fully-connected-layer%EF%BC%89-%E7%A8%A0%E5%AF%86%E5%B1%82%EF%BC%88dense-layer%EF%BC%89"><span class="toc-number">1.3.3.</span> <span class="toc-text">全连接层（fully-connected layer）&#x2F; 稠密层（dense layer）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5%E6%A8%A1%E5%9E%8B%EF%BC%88Embedding%EF%BC%89"><span class="toc-number">1.3.4.</span> <span class="toc-text">嵌入模型（Embedding）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%98%88%E5%80%BC%E5%8D%95%E5%85%83%E8%BF%91%E4%BC%BC%EF%BC%88Threshold-Unit-Approximation%EF%BC%89"><span class="toc-number">1.3.5.</span> <span class="toc-text">阈值单元近似（Threshold Unit Approximation）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-number">1.4.</span> <span class="toc-text">模型训练与优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%EF%BC%88Hyperparameter%EF%BC%89"><span class="toc-number">1.4.1.</span> <span class="toc-text">超参数（Hyperparameter）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E5%8F%82%EF%BC%88hyperparameter-tuning%EF%BC%89"><span class="toc-number">1.4.2.</span> <span class="toc-text">调参（hyperparameter tuning）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E4%B8%8E%E6%B3%9B%E5%8C%96"><span class="toc-number">1.5.</span> <span class="toc-text">模型性能与泛化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%EF%BC%88training-error%EF%BC%89"><span class="toc-number">1.5.1.</span> <span class="toc-text">训练误差（training error）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE%EF%BC%88generalization-error%EF%BC%89"><span class="toc-number">1.5.2.</span> <span class="toc-text">泛化误差（generalization error）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%9B%E5%8C%96%EF%BC%88generalization%EF%BC%89"><span class="toc-number">1.5.3.</span> <span class="toc-text">泛化（generalization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%88overfitting%EF%BC%89"><span class="toc-number">1.5.4.</span> <span class="toc-text">过拟合（overfitting）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88regularization%EF%BC%89"><span class="toc-number">1.5.5.</span> <span class="toc-text">正则化（regularization）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA%E4%B8%8E%E7%BC%96%E7%A0%81"><span class="toc-number">1.6.</span> <span class="toc-text">特征表示与编码</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8B%AC%E7%83%AD%E7%BC%96%E7%A0%81%EF%BC%88one-hot-encoding%EF%BC%89"><span class="toc-number">1.6.1.</span> <span class="toc-text">独热编码（one-hot encoding）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E4%B8%8E%E7%BB%9F%E8%AE%A1%E5%9F%BA%E7%A1%80"><span class="toc-number">1.7.</span> <span class="toc-text">概率与统计基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%EF%BC%88MLE%EF%BC%89"><span class="toc-number">1.7.1.</span> <span class="toc-text">最大似然估计（MLE）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%EF%BC%88Probability%EF%BC%89%E5%92%8C%E4%BC%BC%E7%84%B6%E6%80%A7%EF%BC%88Likelihood%EF%BC%89%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.7.1.1.</span> <span class="toc-text">概率（Probability）和似然性（Likelihood）的区别</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%9F%A5%E8%AF%86%E7%B3%BB%E7%BB%9F"><span class="toc-number">1.8.</span> <span class="toc-text">语言模型与知识系统</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88Large-Language-Model%EF%BC%89"><span class="toc-number">1.8.1.</span> <span class="toc-text">大语言模型（Large Language Model）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E6%8E%92%E5%BA%8F%E5%99%A8%EF%BC%88Reranker%EF%BC%89"><span class="toc-number">1.8.2.</span> <span class="toc-text">重排序器（Reranker）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E5%BA%93-Knowledge-Base%EF%BC%8C%E7%BC%A9%E5%86%99KB"><span class="toc-number">1.8.3.</span> <span class="toc-text">知识库 (Knowledge Base，缩写KB)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%B8%8E%E5%99%AA%E5%A3%B0"><span class="toc-number">1.9.</span> <span class="toc-text">数据与噪声</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%99%AA%E5%A3%B0%EF%BC%88Noise%EF%BC%89"><span class="toc-number">1.9.1.</span> <span class="toc-text">噪声（Noise）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%99%BD%E5%99%AA%E5%A3%B0%EF%BC%88White-Noise%EF%BC%89"><span class="toc-number">1.9.2.</span> <span class="toc-text">白噪声（White Noise）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E8%A1%A5%E5%85%85"><span class="toc-number">1.10.</span> <span class="toc-text">深度学习核心概念补充</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E7%9B%B8%E5%85%B3%EF%BC%88Autocorrelation%EF%BC%89"><span class="toc-number">1.10.1.</span> <span class="toc-text">自相关（Autocorrelation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Convolutional-Neural-Network-CNN%EF%BC%89"><span class="toc-number">1.10.2.</span> <span class="toc-text">卷积神经网络（Convolutional Neural Network, CNN）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%EF%BC%88Convolution%EF%BC%89"><span class="toc-number">1.10.2.1.</span> <span class="toc-text">卷积（Convolution）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%EF%BC%88Pooling%EF%BC%89"><span class="toc-number">1.10.2.2.</span> <span class="toc-text">池化（Pooling）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8%EF%BC%88Markov-Property%EF%BC%89"><span class="toc-number">1.10.3.</span> <span class="toc-text">马尔可夫性质（Markov Property）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Recurrent-Neural-Network-RNN%EF%BC%89"><span class="toc-number">1.10.4.</span> <span class="toc-text">循环神经网络（Recurrent Neural Network, RNN）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%88Long-Short-Term-Memory-LSTM%EF%BC%89"><span class="toc-number">1.10.5.</span> <span class="toc-text">长短期记忆网络（Long Short-Term Memory, LSTM）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83%EF%BC%88Gated-Recurrent-Unit-GRU%EF%BC%89"><span class="toc-number">1.10.6.</span> <span class="toc-text">门控循环单元（Gated Recurrent Unit, GRU）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%EF%BC%88Attention%EF%BC%89"><span class="toc-number">1.10.7.</span> <span class="toc-text">注意力机制（Attention）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformer"><span class="toc-number">1.10.8.</span> <span class="toc-text">Transformer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%EF%BC%88Residual-Connection%EF%BC%89"><span class="toc-number">1.10.9.</span> <span class="toc-text">残差连接（Residual Connection）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout"><span class="toc-number">1.10.10.</span> <span class="toc-text">Dropout</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%B9%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88Batch-Normalization%EF%BC%89"><span class="toc-number">1.10.11.</span> <span class="toc-text">批归一化（Batch Normalization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%88Activation-Function%EF%BC%89"><span class="toc-number">1.10.12.</span> <span class="toc-text">激活函数（Activation Function）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%EF%BC%88Positional-Encoding%EF%BC%89"><span class="toc-number">1.10.13.</span> <span class="toc-text">位置编码（Positional Encoding）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6"><span class="toc-number">2.</span> <span class="toc-text">数学</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E8%88%AC"><span class="toc-number">2.1.</span> <span class="toc-text">一般</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8C%83%E6%95%B0-Norm"><span class="toc-number">2.1.1.</span> <span class="toc-text">范数(Norm)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B0%87"><span class="toc-number">2.1.2.</span> <span class="toc-text">簇</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%95%B0%EF%BC%88Cardinality%EF%BC%89"><span class="toc-number">2.1.3.</span> <span class="toc-text">基数（Cardinality）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%87%A1%E8%A7%A3"><span class="toc-number">2.1.4.</span> <span class="toc-text">平凡解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">线性代数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A1%8C%E5%88%97%E5%BC%8F"><span class="toc-number">2.2.1.</span> <span class="toc-text">行列式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E4%BD%8D%E7%9F%A9%E9%98%B5%EF%BC%88Identity-Matrix%EF%BC%89"><span class="toc-number">2.2.2.</span> <span class="toc-text">单位矩阵（Identity Matrix）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5%EF%BC%88Covariance-Matrix%EF%BC%89"><span class="toc-number">2.2.3.</span> <span class="toc-text">协方差矩阵（Covariance Matrix）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%88%86%E6%AF%8D%E6%98%AF-n-1-%EF%BC%9A%EF%BC%88%E9%80%89%E7%9C%8B%EF%BC%8C%E5%85%AC%E5%BC%8F%E8%83%BD%E6%8E%A8%E4%BD%86%E6%9C%89%E7%82%B9%E8%B4%B9%E5%8A%B2%EF%BC%89"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">为什么分母是 $n - 1$：（选看，公式能推但有点费劲）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC"><span class="toc-number">2.2.4.</span> <span class="toc-text">数学推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%8D%8A%E5%AE%9A%E7%9F%A9%E9%98%B5%EF%BC%88Positive-Semidefinite-Matrix%EF%BC%89"><span class="toc-number">2.2.5.</span> <span class="toc-text">正半定矩阵（Positive Semidefinite Matrix）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="toc-number">2.3.</span> <span class="toc-text">概率论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E7%8E%87%E8%AE%BA"><span class="toc-number">2.3.1.</span> <span class="toc-text">基本概率论</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%EF%BC%88Law-of-Large-Numbers-LLN%EF%BC%89"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">大数定律（Law of Large Numbers, LLN）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%A1%A8%E8%BF%B0%EF%BC%88%E5%BC%B1%E5%A4%A7%E6%95%B0%E5%AE%9A%E5%BE%8B%EF%BC%89"><span class="toc-number">2.3.1.2.</span> <span class="toc-text">数学表述（弱大数定律）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8A%BD%E6%A0%B7%EF%BC%88sampling%EF%BC%89"><span class="toc-number">2.3.2.</span> <span class="toc-text">抽样（sampling）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%9C%9F%E6%9C%9B%EF%BC%88Expectation%EF%BC%8C-E-%EF%BC%89"><span class="toc-number">2.3.3.</span> <span class="toc-text">数学期望（Expectation，$E$）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E5%80%BC%EF%BC%88Sample-Mean%EF%BC%89"><span class="toc-number">2.3.4.</span> <span class="toc-text">均值（Sample Mean）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%B8%83%EF%BC%88distribution%EF%BC%89"><span class="toc-number">2.3.4.1.</span> <span class="toc-text">分布（distribution）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%88%86%E5%B8%83%EF%BC%88multinomial-distribution%EF%BC%89"><span class="toc-number">2.3.4.2.</span> <span class="toc-text">多项分布（multinomial distribution）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Var%EF%BC%88Variance%EF%BC%8C%E6%96%B9%E5%B7%AE%EF%BC%89"><span class="toc-number">2.3.4.3.</span> <span class="toc-text">Var（Variance，方差）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B7%E6%9C%AC%E7%A9%BA%E9%97%B4%EF%BC%88sample-space%EF%BC%89-%E7%BB%93%E6%9E%9C%E7%A9%BA%E9%97%B4%EF%BC%88outcome-space%EF%BC%89"><span class="toc-number">2.3.4.4.</span> <span class="toc-text">样本空间（sample space）&#x2F; 结果空间（outcome space）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8B%E4%BB%B6%EF%BC%88event%EF%BC%89"><span class="toc-number">2.3.4.5.</span> <span class="toc-text">事件（event）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%EF%BC%88probability%EF%BC%89"><span class="toc-number">2.3.4.6.</span> <span class="toc-text">概率（probability）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%88%E9%AA%8C%E6%A6%82%E7%8E%87"><span class="toc-number">2.3.4.7.</span> <span class="toc-text">先验概率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%81%94%E5%90%88%E6%A6%82%E7%8E%87%EF%BC%88joint-probability%EF%BC%89"><span class="toc-number">2.3.4.8.</span> <span class="toc-text">联合概率（joint probability）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%EF%BC%88conditional-probability%EF%BC%89"><span class="toc-number">2.3.4.9.</span> <span class="toc-text">条件概率（conditional probability）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87"><span class="toc-number">2.3.4.10.</span> <span class="toc-text">后验概率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86%EF%BC%88Bayes%E2%80%99-theorem%EF%BC%89"><span class="toc-number">2.3.4.11.</span> <span class="toc-text">贝叶斯定理（Bayes’ theorem）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%B9%E9%99%85%E6%A6%82%E7%8E%87%EF%BC%88marginal-probability%EF%BC%89-%E8%BE%B9%E9%99%85%E5%88%86%E5%B8%83%EF%BC%88marginal-distribution%EF%BC%89"><span class="toc-number">2.3.4.12.</span> <span class="toc-text">边际概率（marginal probability） &#x2F; 边际分布（marginal distribution）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E8%AE%BA%EF%BC%88Information-Theory%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">信息论（Information Theory）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86"><span class="toc-number">3.1.</span> <span class="toc-text">信息论基本定理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A6%99%E5%86%9C%E7%AC%AC%E4%B8%80%E5%AE%9A%E7%90%86%EF%BC%88%E6%97%A0%E5%A4%B1%E7%9C%9F%E7%BC%96%E7%A0%81%E5%AE%9A%E7%90%86%EF%BC%89"><span class="toc-number">3.1.1.</span> <span class="toc-text">香农第一定理（无失真编码定理）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A6%99%E5%86%9C%E7%AC%AC%E4%BA%8C%E5%AE%9A%E7%90%86%EF%BC%88%E4%BF%A1%E9%81%93%E7%BC%96%E7%A0%81%E5%AE%9A%E7%90%86%EF%BC%89"><span class="toc-number">3.1.2.</span> <span class="toc-text">香农第二定理（信道编码定理）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%86%B5%E7%9A%84%E5%AE%9A%E4%B9%89%EF%BC%88entropy%EF%BC%89"><span class="toc-number">3.1.3.</span> <span class="toc-text">熵的定义（entropy）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%92%E4%BF%A1%E6%81%AF"><span class="toc-number">3.1.4.</span> <span class="toc-text">互信息</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="toc-number">3.2.</span> <span class="toc-text">信息论基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E4%BF%A1%E6%81%AF%EF%BC%88Information-Content-Surprisal%EF%BC%89"><span class="toc-number">3.2.1.</span> <span class="toc-text">自信息（Information Content &#x2F; Surprisal）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%EF%BC%88Cross-Entropy%EF%BC%89"><span class="toc-number">3.2.2.</span> <span class="toc-text">交叉熵（Cross-Entropy）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%9C%AF%E8%AF%AD%E4%B8%8E%E5%AE%9A%E4%B9%89%E7%9A%84%E8%8B%B1%E6%96%87"><span class="toc-number">4.</span> <span class="toc-text">常用术语与定义的英文</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E4%B8%AD%E5%B8%B8%E8%A7%81%E5%8F%82%E6%95%B0%E5%90%8D%E5%8F%8A%E8%A7%A3%E9%87%8A%EF%BC%88%E8%8B%B1%E6%96%87%EF%BC%89"><span class="toc-number">5.</span> <span class="toc-text">代码中常见参数名及解释（英文）</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">&copy;2025 By 沉云</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text">愿你的身旁，永远有幸福的魔法相伴</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  let initFn = window.walineFn || null
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const destroyWaline = ele => ele.destroy()

  const initWaline = (Fn, el = document, path = window.location.pathname) => {
    const waline = Fn({
      el: el.querySelector('#waline-wrap'),
      serverURL: 'https://dream-lime.vercel.app',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      comment: false,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    if (isShuoshuo) {
      window.shuoshuoComment.destroyWaline = () => {
        destroyWaline(waline)
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const loadWaline = (el, path) => {
    if (initFn) initWaline(initFn, el, path)
    else {
      btf.getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css')
        .then(() => import('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js'))
        .then(({ init }) => {
          initFn = init || Waline.init
          initWaline(initFn, el, path)
          window.walineFn = initFn
        })
    }
  }

  if (isShuoshuo) {
    'Waline' === 'Waline'
      ? window.shuoshuoComment = { loadComment: loadWaline } 
      : window.loadOtherComment = loadWaline
    return
  }

  if ('Waline' === 'Waline' || !true) {
    if (true) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
    else setTimeout(loadWaline, 0)
  } else {
    window.loadOtherComment = loadWaline
  }
})()</script></div><div class="aplayer no-destroy" data-id="9430428607" data-server="tencent" data-type="playlist" data-fixed="true" data-autoplay="false"> </div><script defer src="https://cdn.jsdelivr.net/npm/sweetalert2@latest/dist/sweetalert2.all.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script>(() => {
  const destroyAplayer = () => {
    if (window.aplayers) {
      for (let i = 0; i < window.aplayers.length; i++) {
        if (!window.aplayers[i].options.fixed) {
          window.aplayers[i].destroy()
        }
      }
    }
  }

  const runMetingJS = () => {
    typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()
  }

  btf.addGlobalFn('pjaxSend', destroyAplayer, 'destroyAplayer')
  btf.addGlobalFn('pjaxComplete', loadMeting, 'runMetingJS')
})()</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      false 
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>