<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>机器学习--入门 | 梦游</title><meta name="author" content="沉云"><meta name="copyright" content="沉云"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习入门">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习--入门">
<meta property="og:url" content="https://na562080.site/2025/07/06/machine-learning-basics.html">
<meta property="og:site_name" content="梦游">
<meta property="og:description" content="机器学习入门">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://na562080.site/img/avatar.webp">
<meta property="article:published_time" content="2025-07-06T06:31:28.000Z">
<meta property="article:modified_time" content="2025-07-21T15:18:37.000Z">
<meta property="article:author" content="沉云">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://na562080.site/img/avatar.webp"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "机器学习--入门",
  "url": "https://na562080.site/2025/07/06/machine-learning-basics.html",
  "image": "https://na562080.site/img/avatar.webp",
  "datePublished": "2025-07-06T06:31:28.000Z",
  "dateModified": "2025-07-21T15:18:37.000Z",
  "author": [
    {
      "@type": "Person",
      "name": "沉云",
      "url": "https://github.com/na562080"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://na562080.site/2025/07/06/machine-learning-basics.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!true && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.json","preload":true,"top_n_per_article":3,"unescape":false,"languages":{"hits_empty":"未找到符合您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习--入门',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/modify.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="梦游" type="application/atom+xml">
</head><body><div id="web_bg" style="background-image: url(/img/Clifftop_Walk_At_Pourville.jpg);"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "/img/loading.gif" data-lazy-src="/img/avatar.webp" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">25</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 画廊</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "/img/loading.gif" data-lazy-src="/img/avatar.webp" alt="Logo"><span class="site-name">梦游</span></a><a class="nav-page-title" href="/"><span class="site-name">机器学习--入门</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 画廊</span></a></div><div class="menus_item"><a class="site-page" href="/shuoshuo/"><i class="fa-fw fas fa-comment-dots"></i><span> 说说</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">机器学习--入门</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-06T06:31:28.000Z" title="发表于 2025-07-06 14:31:28">2025-07-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-21T15:18:37.000Z" title="更新于 2025-07-21 23:18:37">2025-07-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote>
<p>可供参考<br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV164411b7dx/?share_source=copy_web&vd_source=9ad3ed8ff66905792d31539bb9d2ac2d">吴恩达机器学习系列课程</a></p>
</blockquote>
<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>给定一组输入（特征）和对应的输出（标签），训练一个模型来学习输入和输出之间的映射关系。</p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>代价函数是机器学习和优化问题中用来衡量模型预测结果与真实结果之间差异的函数。它反映了模型的“错误”程度——即预测数据相对于真实数据的“损失”，通常通过调整模型参数来最小化代价函数值，从而得到更好预测效果。</p>
<h3 id="1-定义"><a href="#1-定义" class="headerlink" title="1. 定义"></a>1. 定义</h3><p>假设有训练数据集：</p>
<p>$$<br>{(x^{(i)}, y^{(i)})}_{i&#x3D;1}^m<br>$$</p>
<p>其中：</p>
<ul>
<li>$m$ 表示样本数量</li>
<li>$x^{(i)} \in \mathbb{R}^n$ 是第 $i$ 个样本的输入特征向量，包含 $n$ 个特征</li>
<li>$y^{(i)}$ 是第 $i$ 个样本的真实标签（可为标量或向量，视具体任务而定）</li>
</ul>
<p>模型的预测函数为：</p>
<p>$$<br>h_\theta(x^{(i)}) &#x3D; \theta^T x^{(i)} &#x3D; \sum_{j&#x3D;0}^{n} \theta_j x_j^{(i)}<br>$$</p>
<ul>
<li>$\theta$ 表示模型的参数（权重）</li>
</ul>
<p>代价函数（损失函数） $J(\theta)$ 用于衡量模型预测值与真实值之间的差异，通常定义为所有样本的平均误差：</p>
<p>$$<br>J(\theta) &#x3D; \frac{1}{m} \sum_{i&#x3D;1}^m \mathcal{L}\big(h_\theta(x^{(i)}), y^{(i)}\big)<br>$$</p>
<p>$\mathcal{L}(\hat{y}, y)$ 是单个样本的损失函数（loss function），衡量预测值 $\hat{y} &#x3D; h_\theta(x)$ 与真实值 $y$ 的差距。</p>
<h3 id="常见代价函数"><a href="#常见代价函数" class="headerlink" title="常见代价函数"></a>常见代价函数</h3><h4 id="均方误差（Mean-Squared-Error，MSE）"><a href="#均方误差（Mean-Squared-Error，MSE）" class="headerlink" title="均方误差（Mean Squared Error，MSE）"></a>均方误差（Mean Squared Error，MSE）</h4><p>适用于回归问题，定义为预测值与真实值误差的平方的平均值：</p>
<p>$$<br>J(\theta) &#x3D; \frac{1}{2m} \sum_{i&#x3D;1}^m \left(h_\theta(x^{(i)}) - y^{(i)}\right)^2<br>$$</p>
<p>$1&#x2F;2$ 是为了求导时简化表达式。</p>
<h4 id="交叉熵损失（Cross-Entropy-Loss）"><a href="#交叉熵损失（Cross-Entropy-Loss）" class="headerlink" title="交叉熵损失（Cross-Entropy Loss）"></a>交叉熵损失（Cross-Entropy Loss）</h4><p>适用于分类问题，尤其是二分类：</p>
<p>$$<br>J(\theta) &#x3D; -\frac{1}{m} \sum_{i&#x3D;1}^m \left[ y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)})) \right]<br>$$</p>
<p>度量预测概率分布与真实标签的差异。</p>
<ul>
<li><strong>损失函数（Loss Function）</strong> 针对单个样本的误差度量，用来衡量某一个预测值和真实值之间的差距。</li>
<li><strong>代价函数（Cost Function）</strong> 针对整个训练集的平均损失，把所有样本的损失函数求平均或求和。</li>
</ul>
<hr>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降是通过不断调整模型参数使代价函数最小化，找到模型的最优参数。核心思想是沿着代价函数在参数空间中的负梯度方向更新参数，因为梯度指向函数上升最快的方向，沿负梯度方向移动可以最快减小函数值。</p>
<h3 id="公式表示"><a href="#公式表示" class="headerlink" title="公式表示"></a>公式表示</h3><p>假设代价函数为 $J(\theta)$，参数为向量 $\theta$，梯度为 $\nabla_\theta J(\theta)$，则参数更新规则为：</p>
<p>$$<br>\theta :&#x3D; \theta - \alpha \nabla_\theta J(\theta)<br>$$</p>
<ul>
<li>$\alpha$ 是学习率（learning rate），控制每次更新的步长大小。</li>
<li>$\nabla_\theta J(\theta)$ 是代价函数对参数 $\theta$ 的梯度（偏导数组成的向量）。</li>
</ul>
<h3 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h3><ol>
<li>初始化参数 $\theta$（随机或零初始化，神经网络不能为零）。</li>
<li>计算代价函数 $J(\theta)$ 及其梯度 $\nabla_\theta J(\theta)$.</li>
<li>根据梯度下降公式更新参数。</li>
<li>重复步骤 2 和 3，直到代价函数收敛或达到最大迭代次数。</li>
</ol>
<h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><ul>
<li>过大：可能导致参数更新过度（步长过大），甚至发散，无法收敛。</li>
<li>过小：收敛速度慢，训练时间长。</li>
</ul>
<h3 id="参数迭代状态"><a href="#参数迭代状态" class="headerlink" title="参数迭代状态"></a>参数迭代状态</h3><ol>
<li>收敛 (Convergence)<br>参数逐渐靠近某个固定点（极小点），跳动幅度逐渐减小，最终稳定</li>
<li>振荡收敛 (Oscillatory Convergence)<br>参数在目标附近反复跳动，但跳动幅度逐渐减小，最终依然收敛到极小点，常见于学习率较大但未超过临界值的梯度下降。</li>
<li>非收敛振荡 (Non-convergent Oscillation)<br>参数在有限区间内无规则跳动，没有收敛到某个固定点，<strong>但不发散</strong>，属于稳定的但无收敛的状态，类似“电子云”是概率问题。</li>
<li>发散 (Divergence)<br>参数值幅度不断扩大，远离极小点，通常因学习率过大导致训练失败。</li>
</ol>
<h4 id="额外说明（可不看）"><a href="#额外说明（可不看）" class="headerlink" title="额外说明（可不看）"></a>额外说明（可不看）</h4><ul>
<li>梯度下降的收敛条件（以一维二次函数为例）：</li>
</ul>
<p>$$<br>\theta_{t+1} &#x3D; (1 - \alpha a) \theta_t<br>$$</p>
<ul>
<li>若 $|1 - \alpha a| &lt; 1$，收敛  </li>
<li>若 $1 &lt; \alpha a &lt; 2$，震荡收敛  </li>
<li>若 $|1 - \alpha a| &gt; 1$，发散</li>
</ul>
<hr>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>线性回归中可以通过正规方程（Normal Equation）直接计算最优参数 $\theta$，无需使用梯度下降。</p>
<p>假设训练数据集为 $X \in \mathbb{R}^{m \times n}$，目标向量为 $y \in \mathbb{R}^{m}$，其中 $m$ 是样本数量，$n$ 是特征数量。</p>
<ol>
<li><p>损失函数（向量形式）：</p>
<p>$$<br>J(\theta) &#x3D; \frac{1}{2m}(X\theta - y)^T (X\theta - y)<br>$$</p>
</li>
<li><p>对 $\theta$ 求导：</p>
<p>$$<br>\frac{\partial J(\theta)}{\partial \theta} &#x3D; \frac{1}{m} X^T (X\theta - y)<br>$$</p>
</li>
<li><p>令导数为 0，得到最优解条件：</p>
<p>$$<br>X^T (X\theta - y) &#x3D; 0<br>$$</p>
</li>
<li><p>解得正规方程（拆开移项即可）：</p>
<p>$$<br>\theta &#x3D; (X^T X)^{-1} X^T y<br>$$</p>
</li>
</ol>
<ul>
<li>$X^T X$可逆</li>
<li>$\theta$ 是最终预测结果</li>
</ul>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ul>
<li>不需要选择学习率</li>
<li>无需迭代直接求解</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>$(X^T X)^{-1}$ 的时间复杂度为 $O(n^3)$，特征数量较多时效率低下</li>
<li>$X^T X$ 不可逆时无法使用正规方程（可以正则化）</li>
</ul>
<p>为避免不可逆，添加一个微小的正则项 $\lambda I$，形成岭回归：</p>
<p>$$<br>\theta &#x3D; (X^T X + \lambda I)^{-1} X^T y<br>$$</p>
<p>其中 $\lambda$ 是正则化强度，$I$ 是单位矩阵。</p>
<h4 id="为什么添加正则项-lambda-I-可以保证矩阵可逆"><a href="#为什么添加正则项-lambda-I-可以保证矩阵可逆" class="headerlink" title="为什么添加正则项 ( \lambda I ) 可以保证矩阵可逆"></a>为什么添加正则项 ( \lambda I ) 可以保证矩阵可逆</h4><p>在正规方程中，原始需要求逆的矩阵是：</p>
<p>$$<br>X^T X<br>$$</p>
<p>它是一个对称且<strong>正半定</strong>的矩阵，（$x^\top A x \geq 0$则称 $A$ 是一个<strong>正半定矩阵</strong>）意味着它的特征值都是非负的，但可能存在零特征值导致不可逆（部分特征值为零也可能导致矩阵不可逆）。</p>
<p>通过添加正则项，矩阵变为：</p>
<p>$$<br>X^T X + \lambda I<br>$$</p>
<ul>
<li>正则化参数$\lambda &gt; 0$（通常小于1接近0，引入$\lambda$便于调节）</li>
<li>$I$ 是单位矩阵且所有特征值都是 1，加法操作将矩阵的特征值整体“向右平移” $\lambda$ 个单位</li>
</ul>
<p>如果 $X^T X$ 的特征值为 $\sigma_1, \sigma_2, \dots, \sigma_n$，那么$X^T X + \lambda I$的特征值为</p>
<p>$$<br>\sigma_1 + \lambda, \sigma_2 + \lambda, \dots, \sigma_n + \lambda<br>$$</p>
<p>因为 $\lambda &gt; 0$，所以所有特征值都严格大于 0，这意味着 $X^T X + \lambda I$ 是<strong>正定矩阵</strong>，正定矩阵一定是可逆的。最终正规方程的解存在且唯一。</p>
<hr>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>逻辑回归是一种用于<strong>二分类问题</strong>的线性分类模型。与线性回归不同，它的输出是一个概率值，表示某个样本属于正类（label &#x3D; 1）的概率。</p>
<p>逻辑回归的假设函数使用了 sigmoid 函数将线性回归的输出映射到 $(0, 1)$ 区间：</p>
<p>$$<br>h_\theta(x) &#x3D; \frac{1}{1 + e^{-\theta^T x}}<br>$$</p>
<ul>
<li>$x$ 为特征向量</li>
<li>$\theta$ 为模型参数</li>
<li>$h_\theta(x)$ 是预测为正类的概率</li>
</ul>
<h3 id="损失函数（对数损失）"><a href="#损失函数（对数损失）" class="headerlink" title="损失函数（对数损失）"></a>损失函数（对数损失）</h3><p>逻辑回归使用的是<strong>对数损失函数</strong>（Log Loss）：</p>
<p>$$<br>J(\theta) &#x3D; -\frac{1}{m} \sum_{i&#x3D;1}^m \left[ y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)})) \right]<br>$$</p>
<ul>
<li>$m$ 是样本数</li>
<li>$y^{(i)}$ 是第 $i$ 个样本的真实标签</li>
<li>$h_\theta(x^{(i)})$ 是第 $i$ 个样本的预测概率</li>
</ul>
<p>当真实标签 $y^{(i)} &#x3D; 1$ 时，损失为</p>
<p>$$<br>-\log h_\theta(x^{(i)})<br>$$</p>
<p>即预测概率越接近 1，损失越小。</p>
<p>当真实标签 $y^{(i)} &#x3D; 0$ 时，损失为</p>
<p>$$<br>-\log \big(1 - h_\theta(x^{(i)}) \big)<br>$$</p>
<p>即预测概率越接近 0，损失越小。</p>
<p>通常使用<strong>梯度下降</strong>来最小化该损失函数，更新参数 $\theta$。</p>
<h3 id="优点-1"><a href="#优点-1" class="headerlink" title="优点"></a>优点</h3><ul>
<li>训练速度快</li>
</ul>
<h3 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>只能解决线性可分的问题（需要扩展如多项式特征或核方法）</li>
<li>异常值敏感</li>
</ul>
<hr>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>在机器学习中，正则化（Regularization）是一种防止模型过拟合（Overfitting）的技术。通过在损失函数中加入正则项，限制模型参数的复杂度，从而提升模型的泛化能力。</p>
<h3 id="常见的正则化方法"><a href="#常见的正则化方法" class="headerlink" title="常见的正则化方法"></a>常见的正则化方法</h3><h4 id="L2-正则化（岭回归）"><a href="#L2-正则化（岭回归）" class="headerlink" title="L2 正则化（岭回归）"></a>L2 正则化（岭回归）</h4><p>L2 正则化通过惩罚参数的平方和来约束模型：</p>
<p>$$<br>J(\theta) &#x3D; J_0(\theta) + \frac{\lambda}{2m} \sum_{j&#x3D;1}^n \theta_j^2<br>$$</p>
<ul>
<li>$J_0(\theta)$ 是原始损失函数</li>
<li>$\lambda$ 是正则化强度（超参数）</li>
<li>$m$ 是样本数</li>
<li>$\theta_j$ 是第 $j$ 个参数（通常不对 $\theta_0$ 进行正则）</li>
</ul>
<p>L2 正则化使参数趋向于较小的数值，保持模型稳定。</p>
<h4 id="L1-正则化（Lasso）"><a href="#L1-正则化（Lasso）" class="headerlink" title="L1 正则化（Lasso）"></a>L1 正则化（Lasso）</h4><p>L1 正则化通过惩罚参数的绝对值和，具有稀疏性，能实现特征选择：</p>
<p>$$<br>J(\theta) &#x3D; J_0(\theta) + \frac{\lambda}{m} \sum_{j&#x3D;1}^n |\theta_j|<br>$$</p>
<p>L1 正则化会使部分参数变为零，从而简化模型。</p>
<h3 id="正则化在梯度下降中的更新公式"><a href="#正则化在梯度下降中的更新公式" class="headerlink" title="正则化在梯度下降中的更新公式"></a>正则化在梯度下降中的更新公式</h3><p>以 L2 正则化为例，参数更新为：</p>
<p>$$<br>\theta_j :&#x3D; \theta_j - \alpha \left( \frac{1}{m} \sum_{i&#x3D;1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} + \frac{\lambda}{m} \theta_j \right)<br>$$</p>
<ul>
<li>正则化能够有效缓解过拟合问题</li>
<li>L2 正则化偏向参数缩小，但不一定为零</li>
<li>L1 正则化有特征选择功能，使部分参数变为零</li>
</ul>
<hr>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>神经网络（Neural Network）是一种模拟生物神经系统的机器学习模型，由多层神经元（节点）组成，用于拟合复杂的非线性函数。</p>
<h3 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h3><ul>
<li><strong>输入层</strong>：接收输入特征</li>
<li><strong>隐藏层</strong>：一个或多个隐藏层，每层包含若干神经元</li>
<li><strong>输出层</strong>：输出预测结果</li>
</ul>
<p>每个神经元通过加权和加偏置，经过激活函数产生输出。</p>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>对于第 $l$ 层的神经元，假设输入为 $\mathbf{a}^{(l-1)}$，权重矩阵为 $W^{(l)}$，偏置向量为 $\mathbf{b}^{(l)}$，则该层输出为：</p>
<p>$$<br>\mathbf{z}^{(l)} &#x3D; W^{(l)} \mathbf{a}^{(l-1)} + \mathbf{b}^{(l)}<br>$$</p>
<p>激活函数 $g(\cdot)$ 作用后得到激活值：</p>
<p>$$<br>\mathbf{a}^{(l)} &#x3D; g(\mathbf{z}^{(l)})<br>$$</p>
<p>输入层的激活值为输入特征：$\mathbf{a}^{(0)} &#x3D; \mathbf{x}$。</p>
<h4 id="激活函数Sigmoid"><a href="#激活函数Sigmoid" class="headerlink" title="激活函数Sigmoid"></a>激活函数Sigmoid</h4><p>$$<br>\sigma(z) &#x3D; \frac{1}{1 + e^{-z}}<br>$$</p>
<h3 id="损失函数和训练"><a href="#损失函数和训练" class="headerlink" title="损失函数和训练"></a>损失函数和训练</h3><p>通过定义损失函数（如均方误差或交叉熵），利用反向传播算法和梯度下降优化网络参数。</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>从输出层开始，计算每层误差对损失的贡献，逐层传递梯度。</p>
<h3 id="反向传播的数学推导"><a href="#反向传播的数学推导" class="headerlink" title="反向传播的数学推导"></a>反向传播的数学推导</h3><p>假设网络有 $L$ 层，第 $l$ 层输入为 $a^{(l-1)}$，权重矩阵为 $W^{(l)}$，偏置向量为 $b^{(l)}$，激活函数为 $g$，则：</p>
<h3 id="前向计算"><a href="#前向计算" class="headerlink" title="前向计算"></a>前向计算</h3><ul>
<li><p>线性变换：</p>
<p>$$<br>z^{(l)} &#x3D; W^{(l)} a^{(l-1)} + b^{(l)}<br>$$</p>
</li>
<li><p>激活输出：</p>
<p>$$<br>a^{(l)} &#x3D; g(z^{(l)})<br>$$</p>
</li>
</ul>
<h3 id="误差项定义"><a href="#误差项定义" class="headerlink" title="误差项定义"></a>误差项定义</h3><ul>
<li>定义第 $l$ 层误差项：</li>
</ul>
<p>$$<br>\delta^{(l)} &#x3D; \frac{\partial J}{\partial z^{(l)}}<br>$$</p>
<h3 id="输出层误差计算"><a href="#输出层误差计算" class="headerlink" title="输出层误差计算"></a>输出层误差计算</h3><p>最后一层为输出层 $L$，误差为：</p>
<p>$$<br>\delta^{(L)} &#x3D; \nabla_{a^{(L)}} J \odot g’(z^{(L)})<br>$$</p>
<ul>
<li>$\nabla_{a^{(L)}} J$ 是损失对输出的偏导数</li>
<li>$g’(z^{(L)})$ 是激活函数导数</li>
<li>$\odot$ 是元素逐项相乘（Hadamard 积）</li>
</ul>
<h3 id="反向传播误差递推"><a href="#反向传播误差递推" class="headerlink" title="反向传播误差递推"></a>反向传播误差递推</h3><ul>
<li>从输出层往前传播，层 $l$ 的误差：<br>$$<br>\delta^{(l)} &#x3D; (W^{(l+1)})^T \delta^{(l+1)} \odot g’(z^{(l)})<br>$$</li>
</ul>
<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><p>参数梯度计算：</p>
<ul>
<li><p>权重梯度：</p>
<p>$$<br>\frac{\partial J}{\partial W^{(l)}} &#x3D; \delta^{(l)} (a^{(l-1)})^T<br>$$</p>
</li>
<li><p>偏置梯度：</p>
<p>$$<br>\frac{\partial J}{\partial b^{(l)}} &#x3D; \delta^{(l)}<br>$$</p>
</li>
</ul>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_derivative</span>(<span class="params">z</span>):</span><br><span class="line">    s = sigmoid(z)</span><br><span class="line">    <span class="keyword">return</span> s * (<span class="number">1</span> - s)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">X, W1, b1, W2, b2</span>):</span><br><span class="line">    z1 = W1 @ X + b1</span><br><span class="line">    a1 = sigmoid(z1)</span><br><span class="line">    z2 = W2 @ a1 + b2</span><br><span class="line">    a2 = sigmoid(z2)</span><br><span class="line">    <span class="keyword">return</span> z1, a1, z2, a2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">X, Y, z1, a1, z2, a2, W2</span>):</span><br><span class="line">    m = X.shape[<span class="number">1</span>]  <span class="comment"># 样本数</span></span><br><span class="line">    dz2 = a2 - Y</span><br><span class="line">    dW2 = (<span class="number">1</span>/m) * dz2 @ a1.T</span><br><span class="line">    db2 = (<span class="number">1</span>/m) * np.<span class="built_in">sum</span>(dz2, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    dz1 = W2.T @ dz2 * sigmoid_derivative(z1)</span><br><span class="line">    dW1 = (<span class="number">1</span>/m) * dz1 @ X.T</span><br><span class="line">    db1 = (<span class="number">1</span>/m) * np.<span class="built_in">sum</span>(dz1, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dW1, db1, dW2, db2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="不对称性分类的误差评估"><a href="#不对称性分类的误差评估" class="headerlink" title="不对称性分类的误差评估"></a>不对称性分类的误差评估</h2><p>F1 分数是分类问题中常用的性能指标，综合了<strong>精确率</strong>（Precision）和<strong>召回率</strong>（Recall），计算公式如下：</p>
<p>$$<br>\text{Precision} &#x3D; \frac{TP}{TP + FP}<br>$$</p>
<p>$$<br>\text{Recall} &#x3D; \frac{TP}{TP + FN}<br>$$</p>
<p>其中定义：</p>
<p>$$<br>\begin{cases}<br>TP \text{：真正例（True Positive）} \<br>FP \text{：假正例（False Positive）} \<br>FN \text{：假负例（False Negative）}<br>\end{cases}<br>$$</p>
<p>F1 分数定义为精确率和召回率的调和平均数：</p>
<p>$$<br>F_1 &#x3D; 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}<br>$$</p>
<p>这里的乘以 2 来自调和平均数的标准定义，其本质是为了计算两个数的调和平均值。</p>
<p>$$<br>H &#x3D; \frac{2}{\frac{1}{a} + \frac{1}{b}} &#x3D; \frac{2ab}{a + b}<br>$$</p>
<p>调和平均比算术平均略低，更能体现两者不平衡的情况，因此 F1 分数更适合衡量精确率和召回率的平衡。</p>
<hr>
<h2 id="支持向量机（SVM）"><a href="#支持向量机（SVM）" class="headerlink" title="支持向量机（SVM）"></a>支持向量机（SVM）</h2><p>支持向量机是一种强大的监督学习算法，广泛用于分类任务。其核心目标是找到一个最优的决策超平面，将不同类别的样本分开，并且使两类之间的间隔（Margin）最大化。</p>
<h3 id="最大化间隔"><a href="#最大化间隔" class="headerlink" title="最大化间隔"></a>最大化间隔</h3><p>在二分类问题中，找到一个超平面将两类样本分开，并且这个超平面距离训练集中最近的样本点尽可能远，那么模型具有更强的泛化能力。换句话说，最大间隔超平面在训练数据之外更稳健，更不容易被噪声或数据变化影响。</p>
<p>$$<br>\min_{w,b} \frac{1}{2} |w|^2<br>$$</p>
<h4 id="训练样本线性可分：硬间隔（Hard-Margin）最大化"><a href="#训练样本线性可分：硬间隔（Hard-Margin）最大化" class="headerlink" title="训练样本线性可分：硬间隔（Hard Margin）最大化"></a>训练样本线性可分：硬间隔（Hard Margin）最大化</h4><p>即所有正负样本能用一条直线（或高维里的超平面）完全分开，互不混淆，采用硬间隔最大化。该方法要求完全正确地分开样本，不允许任何分类错误。</p>
<h4 id="训练样本近似线性可分：软间隔（Soft-Margin）最大化"><a href="#训练样本近似线性可分：软间隔（Soft-Margin）最大化" class="headerlink" title="训练样本近似线性可分：软间隔（Soft Margin）最大化"></a>训练样本近似线性可分：软间隔（Soft Margin）最大化</h4><p>即大部分样本可以被一条线分开，但少量样本因噪声或偏移无法正确分类时，采用软间隔最大化。<br>该方法在最大化间隔的基础上引入惩罚系数 $C$，以平衡分错样本的损失（惩罚项）和间隔的大小。<br>允许部分样本落在间隔区域内甚至被误分类，以提升模型对新数据的泛化能力。</p>
<h4 id="训练样本线性不可分"><a href="#训练样本线性不可分" class="headerlink" title="训练样本线性不可分"></a>训练样本线性不可分</h4><p>即样本在原始空间中无法被任何超平面线性分开时，使用核技巧将其映射到高维特征空间，在该空间中可能变得线性可分。<br>结合核函数与软间隔最大化训练一个非线性支持向量</p>
<h3 id="决策边界和间隔"><a href="#决策边界和间隔" class="headerlink" title="决策边界和间隔"></a>决策边界和间隔</h3><p>决策边界是分类的分界线（或超平面），表示模型将样本分类为不同类别的标准。<br>间隔是指决策边界与最近样本点之间的距离。</p>
<h3 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h3><p>并不是所有训练样本都决定决策边界，只有距离决策边界最近的那些样本点（称为支持向量）真正影响边界的位置和方向。<br>支持向量使得模型只依赖于少数关键样本，增强了模型的计算效率和泛化能力。</p>
<h3 id="数学表达"><a href="#数学表达" class="headerlink" title="数学表达"></a>数学表达</h3><p>给定训练数据集：</p>
<p>$$<br>{(x^{(i)}, y^{(i)})}_{i&#x3D;1}^m, \quad x^{(i)} \in \mathbb{R}^n, \quad y^{(i)} \in {-1, +1}<br>$$</p>
<p>寻找一个超平面：</p>
<p>$$<br>w^T x + b &#x3D; 0<br>$$</p>
<p>使得对所有样本满足：</p>
<p>$$<br>y^{(i)} (w^T x^{(i)} + b) \geq 1<br>$$</p>
<p>其中，等式成立的样本点是支持向量。</p>
<h3 id="最大化间隔的优化目标"><a href="#最大化间隔的优化目标" class="headerlink" title="最大化间隔的优化目标"></a>最大化间隔的优化目标</h3><p>最大化间隔等价于最小化 $|w|$，可转化为凸二次规划问题：</p>
<p>这里通过约束保证样本分类正确且距离边界不小于1。</p>
<h3 id="为什么只关注支持向量"><a href="#为什么只关注支持向量" class="headerlink" title="为什么只关注支持向量"></a>为什么只关注支持向量</h3><ul>
<li><p><strong>决策边界仅由支持向量唯一确定</strong><br>其他离边界更远的样本点，对边界的影响为零，不会改变优化结果。</p>
</li>
<li><p><strong>减少模型复杂度</strong><br>只用少数支持向量参与模型决策，提升计算效率和存储效率。</p>
</li>
<li><p><strong>增强泛化能力</strong><br>最大间隔原则避免了对所有训练样本的过度拟合，更具鲁棒性。</p>
</li>
</ul>
<p>支持向量机通过最大化间隔，选择最关键的支持向量确定分类边界，实现了一个既简单又强大的分类模型，兼具良好的泛化能力和计算效率。</p>
<hr>
<h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><ul>
<li>很多数据在原始空间线性不可分,映射到一个更高维的空间可能线性可分，但高维空间计算复杂，难以直接实现。核函数能够直接计算两个点在高维映射后对应向量的内积，代替高维映射计算，减少计算量。</li>
<li>核函数计算样本之间的相似度（支持向量机并不需要知道 $\phi(x)$，只需要核函数值）：</li>
</ul>
<p>$$<br>K(x, x’) &#x3D; \phi(x)^T \phi(x’)<br>$$</p>
<ul>
<li>(x, x’)：原始空间中的两个样本点  </li>
<li>(\phi(x), \phi(x’))：将样本映射到高维特征空间后的向量（通常无法显式得到映射函数，代码也不写，不用多考虑）</li>
<li>(K(x, x’))：核函数的值，即映射后向量的内积，表示样本间的相似度</li>
</ul>
<h3 id="高斯核函数的两种常见形式"><a href="#高斯核函数的两种常见形式" class="headerlink" title="高斯核函数的两种常见形式"></a>高斯核函数的两种常见形式</h3><p>高斯核函数常用来衡量两个样本点 $x$ 和 $x’$ 的相似度：</p>
<h3 id="形式一"><a href="#形式一" class="headerlink" title="形式一"></a>形式一</h3><p>$$<br>K(x, x’) &#x3D; \exp\big(-\gamma |x - x’|^2\big)<br>$$</p>
<ul>
<li>$\gamma$ 是核函数的参数，控制“衰减速度”。</li>
</ul>
<h3 id="形式二"><a href="#形式二" class="headerlink" title="形式二"></a>形式二</h3><p>$$<br>K(x, x’) &#x3D; \exp\left(-\frac{|x - x’|^2}{2\sigma^2}\right)<br>$$</p>
<ul>
<li>$\sigma$ 是核函数的带宽参数，控制核函数的宽度，决定了核函数对样本间距离的敏感度，$\sigma$ 越大，核函数的响应越“宽”，远距离样本之间的相似度也越高。</li>
<li>两者关系：</li>
</ul>
<p>$$<br>\gamma &#x3D; \frac{1}{2\sigma^2}<br>$$</p>
<h3 id="对偶目标函数"><a href="#对偶目标函数" class="headerlink" title="对偶目标函数"></a>对偶目标函数</h3><p>$$<br>\max_{\alpha} \sum_{i&#x3D;1}^m \alpha_i - \frac{1}{2} \sum_{i&#x3D;1}^m \sum_{j&#x3D;1}^m \alpha_i \alpha_j y_i y_j K(x_i, x_j)<br>$$</p>
<p>$$<br>\sum_{i&#x3D;1}^m \alpha_i y_i &#x3D; 0, \quad \alpha_i \geq 0, \quad i&#x3D;1,2,\dots,m<br>$$</p>
<ul>
<li>$m$：训练样本总数  </li>
<li>$\alpha_i$：拉格朗日乘子，代表样本点的重要程度，值越大说明对应样本对分类边界贡献越大</li>
<li>$y_i \in {+1, -1}$：第 $i$ 个样本的类别标签  </li>
<li>$x_i$：第 $i$ 个样本的输入特征向量</li>
</ul>
<p>通过核矩阵，算法决定哪些点重要（即支持向量）</p>
<p>重要的点对应的权重$a_i $ 非零，不重要的权重为零，虽然计算了所有点对，但最终模型只使用支持向量做决策</p>
<hr>
<h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><h2 id="K-均值算法（K-Means-Clustering）"><a href="#K-均值算法（K-Means-Clustering）" class="headerlink" title="K 均值算法（K-Means Clustering）"></a>K 均值算法（K-Means Clustering）</h2><p>K 均值是一种<strong>无监督学习</strong>算法，通过将数据划分成 (K) 个簇（群集），使得同一簇内的数据点相似度高，不同簇间差异大。</p>
<h3 id="核心"><a href="#核心" class="headerlink" title="核心"></a>核心</h3><p>随机初始化：一开始随机选取 𝐾 个点作为初始质心（中心点）。</p>
<p>划分阶段：根据距离，把每个点归到最近的质心的簇。（点离哪个质心最近归谁）</p>
<p>更新阶段：计算每个簇所拥有点的均值，把质心移动到这个均值位置。</p>
<p>重复步骤 2 和 3：用新的质心重新划分数据点，再更新质心，直到质心位置基本不变（收敛）或者达到最大迭代次数。</p>
<h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ytusdc/article/details/128504272">归一化 （Normalization）、标准化 （Standardization）和中心&#x2F;零均值化 （Zero-centered）</a></p>
</blockquote>
<p>特征缩放（Feature Scaling）是常用的数据预处理方法，目的是将不同尺度的特征转换到相似的尺度范围内，提升机器学习模型的收敛速度和数值稳定性。</p>
<p>由于标准的 K 均值算法默认所有特征权重相同，不会自动区分哪些特征更重要，每个特征在计算欧氏距离时的“重要性”是一样的。所以在用 K 均值算法前，需要对各特征进行缩放让它们的尺度相似。</p>
<h4 id="Z-score-标准化（标准差标准化）"><a href="#Z-score-标准化（标准差标准化）" class="headerlink" title="Z-score 标准化（标准差标准化）"></a>Z-score 标准化（标准差标准化）</h4><p>$$<br>z &#x3D; \frac{x - \mu}{\sigma}<br>$$</p>
<ul>
<li>(x) 是原始特征值  </li>
<li>(\mu) 是该特征均值  </li>
<li>(\sigma) 是该特征标准差（消除不同特征的波动差异，除样本量没意义）</li>
</ul>
<h4 id="Min-Max-归一化"><a href="#Min-Max-归一化" class="headerlink" title="Min-Max 归一化"></a>Min-Max 归一化</h4><p>$$<br>x’ &#x3D; \frac{x - x_{\min}}{x_{\max} - x_{\min}}<br>$$</p>
<ul>
<li>$x$ 是原始特征值</li>
<li>$x_{\min}$ 是该特征的最小值</li>
<li>$x_{\max}$ 是该特征的最大值</li>
<li>$x’$ 是归一化后的特征值，范围在 $[0, 1]$</li>
</ul>
<h4 id="均值归一化（Mean-Normalization）"><a href="#均值归一化（Mean-Normalization）" class="headerlink" title="均值归一化（Mean Normalization）"></a>均值归一化（Mean Normalization）</h4><p>$$<br>x’ &#x3D; \frac{x - \bar{x}}{x_{\max} - x_{\min}}<br>$$</p>
<ul>
<li>参数同上  </li>
<li>$x’$ 范围变为 $[-1, 1]$ 附近（可能超出，总体趋势在范围内，和极值有关）</li>
</ul>
<h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>将数据集分成 (K) 个簇，最小化簇内点到簇中心的距离平方和：</p>
<p>$$<br>J &#x3D; \sum_{k&#x3D;1}^K \sum_{x_i \in C_k} | x_i - \mu_k |^2<br>$$</p>
<ul>
<li>$C_k$：第 $k$ 个簇的所有点集合</li>
<li>$\mu_k$：第 $k$ 个簇的质心（均值）</li>
</ul>
<h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol>
<li><p><strong>初始化</strong>：随机选择 $K$ 个点作为初始簇中心 $\mu_k$。</p>
</li>
<li><p><strong>分配步骤</strong>：将每个点分配给距离最近的簇中心：</p>
</li>
</ol>
<p>$$<br>c_i &#x3D; \arg\min_{k} | x_i - \mu_k |^2<br>$$</p>
<ol start="3">
<li><strong>更新步骤</strong>：重新计算每个簇的质心：</li>
</ol>
<p>$$<br>\mu_k &#x3D; \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i<br>$$</p>
<ul>
<li>arg min 表示函数取得最小值时对应的变量取值</li>
</ul>
<ol start="4">
<li><strong>重复步骤 2 和 3</strong>，直到簇分配不再变化或达到最大迭代次数。</li>
</ol>
<h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">4</span>], [<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">              [<span class="number">4</span>, <span class="number">2</span>], [<span class="number">4</span>, <span class="number">4</span>], [<span class="number">4</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型，指定簇数，random_state =0固定随机结果，产生相同的随机数序列</span></span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练模型(标准化后)</span></span><br><span class="line">kmeans.fit(X_scaled)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 簇中心</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;簇中心：&quot;</span>, kmeans.cluster_centers_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预测每个点所属簇</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;簇标签：&quot;</span>, kmeans.labels_)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line"></span><br><span class="line">def kmeans(X, num_clusters=2, num_iters=100, device=&#x27;cpu&#x27;):</span><br><span class="line">    X = X.to(device)</span><br><span class="line"></span><br><span class="line">    # 随机初始化质心</span><br><span class="line">    indices = torch.randperm(X.size(0))[:num_clusters]</span><br><span class="line">    centroids = X[indices]</span><br><span class="line"></span><br><span class="line">    for _ in range(num_iters):</span><br><span class="line">        # 计算每个点到每个质心的距离</span><br><span class="line">        distances = torch.cdist(X, centroids)  # [num_points, num_clusters]</span><br><span class="line"></span><br><span class="line">        # 分配标签为最近质心索引</span><br><span class="line">        labels = torch.argmin(distances, dim=1)</span><br><span class="line"></span><br><span class="line">        new_centroids = []</span><br><span class="line">        for k in range(num_clusters):</span><br><span class="line">            assigned = X[labels == k]</span><br><span class="line">            if assigned.size(0) == 0:</span><br><span class="line">                # 如果该簇没有点，随机重新初始化质心</span><br><span class="line">                new_centroids.append(X[torch.randint(0, X.size(0), (1,))].squeeze(0))</span><br><span class="line">            else:</span><br><span class="line">                new_centroids.append(assigned.mean(dim=0))</span><br><span class="line">        new_centroids = torch.stack(new_centroids)</span><br><span class="line"></span><br><span class="line">        # 判断质心是否收敛</span><br><span class="line">        if torch.allclose(centroids, new_centroids, atol=1e-4):</span><br><span class="line">            break</span><br><span class="line"></span><br><span class="line">        centroids = new_centroids</span><br><span class="line"></span><br><span class="line">    return centroids, labels</span><br><span class="line"></span><br><span class="line"># 示例用法</span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    X = torch.tensor([[1., 2.], [1., 4.], [1., 0.],</span><br><span class="line">                      [4., 2.], [4., 4.], [4., 0.]])</span><br><span class="line">    centroids, labels = kmeans(X, num_clusters=2)</span><br><span class="line">    print(&quot;簇中心：&quot;, centroids)</span><br><span class="line">    print(&quot;簇标签：&quot;, labels)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="肘部算法（Elbow-Method）"><a href="#肘部算法（Elbow-Method）" class="headerlink" title="肘部算法（Elbow Method）"></a>肘部算法（Elbow Method）</h3><p>肘部算法是用于确定 KMeans 聚类中最佳簇数 $K$ 的一种经典算法。</p>
<h4 id="每个簇的质心"><a href="#每个簇的质心" class="headerlink" title="每个簇的质心"></a>每个簇的质心</h4><p>对于第 $k$ 个簇 $C_k$，其质心 $\mu_k$ 定义为：</p>
<p>$$<br>\mu_k &#x3D; \frac{1}{|C_k|} \sum_{x_i \in C_k} x_i<br>$$</p>
<ul>
<li>$C_k$：第 $k$ 个簇的样本集合</li>
<li>$|C_k|$：第 $k$ 个簇的样本个数（是集合的基数，不是绝对值！！）</li>
<li>$x_i$：属于第 $k$ 个簇中的样本点</li>
</ul>
<p>根据可视图视觉判断拐点</p>
<hr>
<h3 id="主成分分析（PCA）"><a href="#主成分分析（PCA）" class="headerlink" title="主成分分析（PCA）"></a>主成分分析（PCA）</h3><p>PCA 的目标是找到一个低维子空间，使数据投影到这个子空间后具有最大方差，尽可能保留原始数据的信息量。</p>
<p>它通过寻找一组新的正交基（主成分）来重新表达数据。这些主成分按照方差大小排序。<br>（第一主成分是方差最大的方向，第二主成分与第一主成分正交，方差次大，依此类推）</p>
<p>简单来说，PCA 在做一组新的特征加权和，这个加权通过线性代数自动计算得出，目的是让数据在新坐标轴上的分布尽可能分散，从而尽可能保留原始数据的结构和信息。</p>
<h3 id="数据中心化（Data-Centering）"><a href="#数据中心化（Data-Centering）" class="headerlink" title="数据中心化（Data Centering）"></a>数据中心化（Data Centering）</h3><p>数据中心化的目的是将数据的参考线（基准线）从原点移动到样本均值，处理后能更清晰地反映数据的波动和差异。</p>
<p>对于第 $j$ 个维度，所有样本的均值计算公式为：</p>
<p>$$<br>\bar{x}<em>j &#x3D; \frac{1}{n} \sum</em>{i&#x3D;1}^{n} x_{ij}<br>$$</p>
<ul>
<li>$n$：样本总数</li>
<li>$x_{ij}$：第 $i$ 个样本第 $j$ 个维度的取值</li>
</ul>
<p>将每个样本的该维度值减去均值，得到中心化后的数据：</p>
<p>$$<br>x_{ij}^{centered} &#x3D; x_{ij} - \bar{x}_j<br>$$</p>
<p>处理后，该维度所有样本的均值变为 0（就是平均值为 0）。</p>
<h3 id="异常检测算法"><a href="#异常检测算法" class="headerlink" title="异常检测算法"></a>异常检测算法</h3><p>异常检测的目标：不是为了学会“异常长什么样”，而是学会“什么是正常”。所以遇到异常样本不是判断样本异常，而是非正常。因此异常检测算法允许甚至默认异常样本极少甚至没有。</p>
<h4 id="轴对轴独立误判问题"><a href="#轴对轴独立误判问题" class="headerlink" title="轴对轴独立误判问题"></a>轴对轴独立误判问题</h4><p>异常点在整体空间异常，但在每个轴上的投影看起来“正常”，如果算法只考虑单变量的概率（如高斯分布），会误判为正常点。</p>
<h4 id="多变量高斯分布"><a href="#多变量高斯分布" class="headerlink" title="多变量高斯分布"></a>多变量高斯分布</h4><p>多变量高斯分布是 一组变量的联合概率分布，它不仅考虑每个变量的单独分布，还考虑它们之间的相关性。<br>有几个特征（feature），就需要几维的协方差矩阵。</p>
<hr>
<h2 id="协同过滤算法"><a href="#协同过滤算法" class="headerlink" title="协同过滤算法"></a>协同过滤算法</h2><p>协同过滤（Collaborative Filtering，CF）是一种基于历史行为数据来进行推荐的算法，广泛应用于推荐系统中。其核心思想是通过用户之间或物品之间的相似度，来预测用户可能感兴趣的物品。</p>
<ol>
<li>表示交互数据，构建稀疏矩阵 $R \in \mathbb{R}^{|U| \times |I|}$</li>
<li>计算相似度</li>
<li>邻域选择，选取最相似的 $k$ 个邻域。</li>
<li>偏好预测，聚合邻域信息，估计预期评分。</li>
</ol>
<h3 id="缺点-2"><a href="#缺点-2" class="headerlink" title="缺点"></a>缺点</h3><ul>
<li>稀疏性问题：用户-物品矩阵通常非常稀疏，影响相似度计算准确性。</li>
<li>冷启动问题：对新用户或新物品缺乏足够数据难以推荐。</li>
<li>计算成本高：数据过大时相似度计算和搜索开销大。</li>
</ul>
<h3 id="公式（非重点）"><a href="#公式（非重点）" class="headerlink" title="公式（非重点）"></a>公式（非重点）</h3><ul>
<li><p><strong>余弦相似度（User-Based）</strong></p>
<p>$$<br>\text{sim}(u,v) &#x3D; \frac{\sum_{i \in I_{uv}} r_{u,i} \times r_{v,i}}{\sqrt{\sum_{i \in I_u} r_{u,i}^2} \times \sqrt{\sum_{i \in I_v} r_{v,i}^2}}<br>$$</p>
<p>其中，$r_{u,i}$ 是用户 $u$ 对物品 $i$ 的评分，$I_u$ 是用户 $u$ 评分过的物品集合，$I_{uv} &#x3D; I_u \cap I_v$。</p>
</li>
<li><p><strong>预测评分</strong></p>
<p>$$<br>\hat{r}<em>{u,j} &#x3D; \frac{\sum</em>{v \in N(u)} \text{sim}(u,v) \times r_{v,j}}{\sum_{v \in N(u)} |\text{sim}(u,v)|}<br>$$</p>
<p>其中，$N(u)$ 是用户 $u$ 的邻居集合，$r_{v,j}$ 是邻居 $v$ 对物品 $j$ 的评分。</p>
</li>
</ul>
<hr>
<h2 id="大数据集处理"><a href="#大数据集处理" class="headerlink" title="大数据集处理"></a>大数据集处理</h2><h3 id="随机梯度下降（Stochastic-Gradient-Descent，SGD）"><a href="#随机梯度下降（Stochastic-Gradient-Descent，SGD）" class="headerlink" title="随机梯度下降（Stochastic Gradient Descent，SGD）"></a>随机梯度下降（Stochastic Gradient Descent，SGD）</h3><p>随机梯度下降是一种优化算法，用于通过迭代方式最小化目标函数（如损失函数），尤其适合大规模数据训练。</p>
<h4 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h4><ul>
<li>与批量梯度下降（Batch Gradient Descent）每次使用全部训练样本计算梯度不同，随机梯度下降每次只用一个样本来计算梯度并更新参数。</li>
</ul>
<h4 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h4><p>假设模型参数为 $\theta$，学习率为 $\alpha$，训练集共有 $m$ 个样本：</p>
<ol>
<li><p>初始化参数 $\theta$</p>
</li>
<li><p>对于每个训练样本 $(x^{(i)}, y^{(i)})$：</p>
<ul>
<li><p>计算梯度：  </p>
<p>$$<br>g_i &#x3D; \nabla_\theta L(h_\theta(x^{(i)}), y^{(i)})<br>$$</p>
</li>
<li><p>更新参数：  </p>
<p>$$<br>\theta :&#x3D; \theta - \alpha g_i<br>$$</p>
</li>
</ul>
</li>
<li><p>重复多轮迭代，直到收敛</p>
</li>
</ol>
<h4 id="优点-2"><a href="#优点-2" class="headerlink" title="优点"></a>优点</h4><ul>
<li>计算速度快，每次更新只需用一个样本</li>
<li>能跳出局部极小值，适合非凸优化</li>
<li>适合在线学习，能处理动态数据</li>
</ul>
<h4 id="缺点-3"><a href="#缺点-3" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>更新过程带噪声，收敛不如批量梯度下降稳定</li>
<li>收敛路径可能震荡，需采用学习率衰减或优化技巧</li>
</ul>
<h3 id="Mini-Batch-梯度下降"><a href="#Mini-Batch-梯度下降" class="headerlink" title="Mini-Batch 梯度下降"></a>Mini-Batch 梯度下降</h3><p>Mini-Batch 梯度下降是批量梯度下降和随机梯度下降之间的一种折中方法。</p>
<h4 id="基本思想-1"><a href="#基本思想-1" class="headerlink" title="基本思想"></a>基本思想</h4><ul>
<li>将训练数据划分成若干个小批次（mini-batch），每个小批次包含一定数量的样本。</li>
<li>每次使用一个 mini-batch 计算梯度并更新参数。</li>
</ul>
<h4 id="算法步骤-2"><a href="#算法步骤-2" class="headerlink" title="算法步骤"></a>算法步骤</h4><p>假设参数为 $\theta$，学习率为 $\alpha$，数据集共有 $m$ 个样本，mini-batch 大小为 $b$：</p>
<ol>
<li>将训练集划分为 $\lceil m &#x2F; b \rceil$ 个 mini-batch。</li>
<li>对每个 mini-batch：<ul>
<li><p>计算该批次的平均梯度：  </p>
<p>$$<br>g &#x3D; \frac{1}{b} \sum_{i&#x3D;1}^b \nabla_\theta L(h_\theta(x^{(i)}), y^{(i)})<br>$$</p>
</li>
<li><p>更新参数：  </p>
<p>$$<br>\theta :&#x3D; \theta - \alpha g<br>$$</p>
</li>
</ul>
</li>
</ol>
<h4 id="优点-3"><a href="#优点-3" class="headerlink" title="优点"></a>优点</h4><p>计算效率高，梯度估计更准确，训练更稳定，可优化批量计算</p>
<h4 id="缺点-4"><a href="#缺点-4" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>需要选择合适的 mini-batch 大小，大小不合适会影响性能</li>
<li>仍有一定的噪声，可能导致收敛震荡</li>
</ul>
<h3 id="随机梯度下降收敛"><a href="#随机梯度下降收敛" class="headerlink" title="随机梯度下降收敛"></a>随机梯度下降收敛</h3><p>随机梯度下降（SGD）由于每次只用一个样本计算梯度，参数更新带有噪声，因此其收敛性质与批量梯度下降不同。</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li><strong>噪声性质</strong>：SGD 更新方向是梯度的无偏估计，但含有随机噪声，导致参数在最优值附近振荡。</li>
<li><strong>收敛速度</strong>：初期收敛较快，但后期由于噪声，难以完全收敛到精确的最优点。</li>
<li><strong>学习率衰减</strong>：通过逐渐减小学习率，可以减小振荡幅度，使参数更接近最优值，从而保证收敛。</li>
</ul>
<h4 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h4><ul>
<li>在凸函数和合适的学习率条件下，SGD 几乎必然收敛到全局最优解。</li>
<li>在非凸问题（如深度神经网络）中，SGD 有能力跳出局部极小值，找到较好解。</li>
</ul>
<p>虽然 SGD 本质带噪声，但通过合理设计学习率和优化技巧，可以实现稳定且高效的模型训练收敛。</p>
<h3 id="Map-Reduce-中的“减少映射”与“数据并行”"><a href="#Map-Reduce-中的“减少映射”与“数据并行”" class="headerlink" title="Map-Reduce 中的“减少映射”与“数据并行”"></a>Map-Reduce 中的“减少映射”与“数据并行”</h3><p>在使用 Map-Reduce 模型处理大规模数据时，有两个核心思想帮助提高效率和扩展性：</p>
<h4 id="减少映射（Reducing-Mapping）"><a href="#减少映射（Reducing-Mapping）" class="headerlink" title="减少映射（Reducing Mapping）"></a>减少映射（Reducing Mapping）</h4><p>在分布式数据处理中，每个节点独立处理一部分数据，生成大量中间结果。为减轻后续合并和计算的压力，可以在本地尽量合并相同的结果或者过滤无关数据，减少传输的数据量。</p>
<p>这种减少是对中间结果进行本地的预聚合或筛选，保证传递给下一阶段的数据包含所有必要的信息，不会影响最终计算结果的正确性。</p>
<h4 id="数据并行（Data-Parallelism）"><a href="#数据并行（Data-Parallelism）" class="headerlink" title="数据并行（Data Parallelism）"></a>数据并行（Data Parallelism）</h4><p>将大数据集划分成多个子块，在不同的计算节点或线程上独立执行相同的 Map 或 Reduce 操作，从而实现并行处理。</p>
<p>同一函数并行处理不同数据子集，计算过程中节点彼此独立无需通信（完成后要汇总），易扩展计算资源（简而言之方便加入增加计算节点而不影响已有系统的运行和架构）。</p>
<hr>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><h2 id="权重矩阵"><a href="#权重矩阵" class="headerlink" title="权重矩阵"></a>权重矩阵</h2><blockquote>
<p>可参考<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/qilei2010/article/details/106117322">神经网络中的矩阵的理解</a></p>
</blockquote>
<p>权重矩阵的每行对应一个因素。每行的数字是作用于某个因素的所有权重。<br>权重矩阵的每列对应一个结果。每列的数字是影响某个结果的全部权重。</p>
<details class="toggle" style="border: 1px solid rgba(255"><summary class="toggle-button" style="background-color: rgba(255;color:  255">错误理解</summary><div class="toggle-content"><p>以前一直在宏大的想一个对应一个，很混乱，不明白想不通，把权重矩阵的所有点视作独立点，因为每行对应乘了全部输入量，就是不清楚为什么一个因素必须被所有输入影响。</p>
<p>每行是一个因素的所有影响点，每列是一个点的所有影响因素，每行因素之间没有任何联系，也互不影响，没有任何的自然意义，不要人为赋予意义。</p>
</div></details>





















</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/na562080">沉云</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://na562080.site/2025/07/06/machine-learning-basics.html">https://na562080.site/2025/07/06/machine-learning-basics.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://na562080.site" target="_blank">梦游</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.</span> <span class="toc-text">代价函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.1.</span> <span class="toc-text">1. 定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.2.</span> <span class="toc-text">常见代价函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%EF%BC%88Mean-Squared-Error%EF%BC%8CMSE%EF%BC%89"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">均方误差（Mean Squared Error，MSE）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%EF%BC%88Cross-Entropy-Loss%EF%BC%89"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">交叉熵损失（Cross-Entropy Loss）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">1.2.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.2.1.</span> <span class="toc-text">公式表示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E7%A8%8B"><span class="toc-number">1.2.2.</span> <span class="toc-text">过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">1.2.3.</span> <span class="toc-text">学习率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E8%BF%AD%E4%BB%A3%E7%8A%B6%E6%80%81"><span class="toc-number">1.2.4.</span> <span class="toc-text">参数迭代状态</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%9D%E5%A4%96%E8%AF%B4%E6%98%8E%EF%BC%88%E5%8F%AF%E4%B8%8D%E7%9C%8B%EF%BC%89"><span class="toc-number">1.2.4.1.</span> <span class="toc-text">额外说明（可不看）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-number">1.3.</span> <span class="toc-text">正规方程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">1.3.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">1.3.2.</span> <span class="toc-text">缺点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E6%B7%BB%E5%8A%A0%E6%AD%A3%E5%88%99%E9%A1%B9-lambda-I-%E5%8F%AF%E4%BB%A5%E4%BF%9D%E8%AF%81%E7%9F%A9%E9%98%B5%E5%8F%AF%E9%80%86"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">为什么添加正则项 ( \lambda I ) 可以保证矩阵可逆</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">1.4.</span> <span class="toc-text">逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88%E5%AF%B9%E6%95%B0%E6%8D%9F%E5%A4%B1%EF%BC%89"><span class="toc-number">1.4.1.</span> <span class="toc-text">损失函数（对数损失）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-1"><span class="toc-number">1.4.2.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-1"><span class="toc-number">1.4.3.</span> <span class="toc-text">缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">1.5.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">1.5.1.</span> <span class="toc-text">常见的正则化方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#L2-%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88%E5%B2%AD%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="toc-number">1.5.1.1.</span> <span class="toc-text">L2 正则化（岭回归）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#L1-%E6%AD%A3%E5%88%99%E5%8C%96%EF%BC%88Lasso%EF%BC%89"><span class="toc-number">1.5.1.2.</span> <span class="toc-text">L1 正则化（Lasso）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%9C%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%AD%E7%9A%84%E6%9B%B4%E6%96%B0%E5%85%AC%E5%BC%8F"><span class="toc-number">1.5.2.</span> <span class="toc-text">正则化在梯度下降中的更新公式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.6.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%BB%93%E6%9E%84"><span class="toc-number">1.6.1.</span> <span class="toc-text">基本结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.6.2.</span> <span class="toc-text">前向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0Sigmoid"><span class="toc-number">1.6.2.1.</span> <span class="toc-text">激活函数Sigmoid</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E8%AE%AD%E7%BB%83"><span class="toc-number">1.6.3.</span> <span class="toc-text">损失函数和训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.7.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC"><span class="toc-number">1.7.1.</span> <span class="toc-text">反向传播的数学推导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97"><span class="toc-number">1.7.2.</span> <span class="toc-text">前向计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E9%A1%B9%E5%AE%9A%E4%B9%89"><span class="toc-number">1.7.3.</span> <span class="toc-text">误差项定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%B1%82%E8%AF%AF%E5%B7%AE%E8%AE%A1%E7%AE%97"><span class="toc-number">1.7.4.</span> <span class="toc-text">输出层误差计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%AF%E5%B7%AE%E9%80%92%E6%8E%A8"><span class="toc-number">1.7.5.</span> <span class="toc-text">反向传播误差递推</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">1.7.6.</span> <span class="toc-text">梯度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.7.7.</span> <span class="toc-text">代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%AF%B9%E7%A7%B0%E6%80%A7%E5%88%86%E7%B1%BB%E7%9A%84%E8%AF%AF%E5%B7%AE%E8%AF%84%E4%BC%B0"><span class="toc-number">1.8.</span> <span class="toc-text">不对称性分类的误差评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89"><span class="toc-number">1.9.</span> <span class="toc-text">支持向量机（SVM）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E5%8C%96%E9%97%B4%E9%9A%94"><span class="toc-number">1.9.1.</span> <span class="toc-text">最大化间隔</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%EF%BC%9A%E7%A1%AC%E9%97%B4%E9%9A%94%EF%BC%88Hard-Margin%EF%BC%89%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="toc-number">1.9.1.1.</span> <span class="toc-text">训练样本线性可分：硬间隔（Hard Margin）最大化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E8%BF%91%E4%BC%BC%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86%EF%BC%9A%E8%BD%AF%E9%97%B4%E9%9A%94%EF%BC%88Soft-Margin%EF%BC%89%E6%9C%80%E5%A4%A7%E5%8C%96"><span class="toc-number">1.9.1.2.</span> <span class="toc-text">训练样本近似线性可分：软间隔（Soft Margin）最大化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A0%B7%E6%9C%AC%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86"><span class="toc-number">1.9.1.3.</span> <span class="toc-text">训练样本线性不可分</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C%E5%92%8C%E9%97%B4%E9%9A%94"><span class="toc-number">1.9.2.</span> <span class="toc-text">决策边界和间隔</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="toc-number">1.9.3.</span> <span class="toc-text">支持向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E8%A1%A8%E8%BE%BE"><span class="toc-number">1.9.4.</span> <span class="toc-text">数学表达</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E5%8C%96%E9%97%B4%E9%9A%94%E7%9A%84%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-number">1.9.5.</span> <span class="toc-text">最大化间隔的优化目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AA%E5%85%B3%E6%B3%A8%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F"><span class="toc-number">1.9.6.</span> <span class="toc-text">为什么只关注支持向量</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">1.10.</span> <span class="toc-text">核函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AB%98%E6%96%AF%E6%A0%B8%E5%87%BD%E6%95%B0%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%B8%B8%E8%A7%81%E5%BD%A2%E5%BC%8F"><span class="toc-number">1.10.1.</span> <span class="toc-text">高斯核函数的两种常见形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%A2%E5%BC%8F%E4%B8%80"><span class="toc-number">1.10.2.</span> <span class="toc-text">形式一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%A2%E5%BC%8F%E4%BA%8C"><span class="toc-number">1.10.3.</span> <span class="toc-text">形式二</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E5%81%B6%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="toc-number">1.10.4.</span> <span class="toc-text">对偶目标函数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.</span> <span class="toc-text">无监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#K-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95%EF%BC%88K-Means-Clustering%EF%BC%89"><span class="toc-number">2.1.</span> <span class="toc-text">K 均值算法（K-Means Clustering）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83"><span class="toc-number">2.1.1.</span> <span class="toc-text">核心</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">2.1.2.</span> <span class="toc-text">特征缩放</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Z-score-%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%88%E6%A0%87%E5%87%86%E5%B7%AE%E6%A0%87%E5%87%86%E5%8C%96%EF%BC%89"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">Z-score 标准化（标准差标准化）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Min-Max-%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">Min-Max 归一化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9D%87%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88Mean-Normalization%EF%BC%89"><span class="toc-number">2.1.2.3.</span> <span class="toc-text">均值归一化（Mean Normalization）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.3.</span> <span class="toc-text">目标函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="toc-number">2.1.4.</span> <span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-number">2.1.5.</span> <span class="toc-text">代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%82%98%E9%83%A8%E7%AE%97%E6%B3%95%EF%BC%88Elbow-Method%EF%BC%89"><span class="toc-number">2.1.6.</span> <span class="toc-text">肘部算法（Elbow Method）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%AF%8F%E4%B8%AA%E7%B0%87%E7%9A%84%E8%B4%A8%E5%BF%83"><span class="toc-number">2.1.6.1.</span> <span class="toc-text">每个簇的质心</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89"><span class="toc-number">2.1.7.</span> <span class="toc-text">主成分分析（PCA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E5%8C%96%EF%BC%88Data-Centering%EF%BC%89"><span class="toc-number">2.1.8.</span> <span class="toc-text">数据中心化（Data Centering）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95"><span class="toc-number">2.1.9.</span> <span class="toc-text">异常检测算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BD%B4%E5%AF%B9%E8%BD%B4%E7%8B%AC%E7%AB%8B%E8%AF%AF%E5%88%A4%E9%97%AE%E9%A2%98"><span class="toc-number">2.1.9.1.</span> <span class="toc-text">轴对轴独立误判问题</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83"><span class="toc-number">2.1.9.2.</span> <span class="toc-text">多变量高斯分布</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">协同过滤算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-2"><span class="toc-number">2.2.1.</span> <span class="toc-text">缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F%EF%BC%88%E9%9D%9E%E9%87%8D%E7%82%B9%EF%BC%89"><span class="toc-number">2.2.2.</span> <span class="toc-text">公式（非重点）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86"><span class="toc-number">2.3.</span> <span class="toc-text">大数据集处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Stochastic-Gradient-Descent%EF%BC%8CSGD%EF%BC%89"><span class="toc-number">2.3.1.</span> <span class="toc-text">随机梯度下降（Stochastic Gradient Descent，SGD）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="toc-number">2.3.1.1.</span> <span class="toc-text">基本思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-1"><span class="toc-number">2.3.1.2.</span> <span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-2"><span class="toc-number">2.3.1.3.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-3"><span class="toc-number">2.3.1.4.</span> <span class="toc-text">缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mini-Batch-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.3.2.</span> <span class="toc-text">Mini-Batch 梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-1"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">基本思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4-2"><span class="toc-number">2.3.2.2.</span> <span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%82%B9-3"><span class="toc-number">2.3.2.3.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9-4"><span class="toc-number">2.3.2.4.</span> <span class="toc-text">缺点</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B"><span class="toc-number">2.3.3.</span> <span class="toc-text">随机梯度下降收敛</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E7%82%B9"><span class="toc-number">2.3.3.1.</span> <span class="toc-text">特点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%90%86%E8%AE%BA"><span class="toc-number">2.3.3.2.</span> <span class="toc-text">理论</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Map-Reduce-%E4%B8%AD%E7%9A%84%E2%80%9C%E5%87%8F%E5%B0%91%E6%98%A0%E5%B0%84%E2%80%9D%E4%B8%8E%E2%80%9C%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%E2%80%9D"><span class="toc-number">2.3.4.</span> <span class="toc-text">Map-Reduce 中的“减少映射”与“数据并行”</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%87%8F%E5%B0%91%E6%98%A0%E5%B0%84%EF%BC%88Reducing-Mapping%EF%BC%89"><span class="toc-number">2.3.4.1.</span> <span class="toc-text">减少映射（Reducing Mapping）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%EF%BC%88Data-Parallelism%EF%BC%89"><span class="toc-number">2.3.4.2.</span> <span class="toc-text">数据并行（Data Parallelism）</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%A5%E5%85%85"><span class="toc-number">3.</span> <span class="toc-text">补充</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E7%9F%A9%E9%98%B5"><span class="toc-number">3.1.</span> <span class="toc-text">权重矩阵</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: transparent;"><div id="footer-wrap"><div class="copyright">&copy;2025 By 沉云</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.5</a></div><div class="footer_custom_text">愿你的身旁，永远有幸福的魔法相伴</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(() => {
  const loadMathjax = () => {
    if (!window.MathJax) {
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          tags: 'none',
        },
        chtml: {
          scale: 1.1
        },
        options: {
          enableMenu: true,
          renderActions: {
            findScript: [10, doc => {
              for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
                const display = !!node.type.match(/; *mode=display/)
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
                const text = document.createTextNode('')
                node.parentNode.replaceChild(text, node)
                math.start = {node: text, delim: '', n: 0}
                math.end = {node: text, delim: '', n: 0}
                doc.math.push(math)
              }
            }, '']
          }
        }
      }

      const script = document.createElement('script')
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
      script.id = 'MathJax-script'
      script.async = true
      document.head.appendChild(script)
    } else {
      MathJax.startup.document.state(0)
      MathJax.texReset()
      MathJax.typesetPromise()
    }
  }

  btf.addGlobalFn('encrypt', loadMathjax, 'mathjax')
  window.pjax ? loadMathjax() : window.addEventListener('load', loadMathjax)
})()</script><script>(() => {
  let initFn = window.walineFn || null
  const isShuoshuo = GLOBAL_CONFIG_SITE.pageType === 'shuoshuo'
  const option = null

  const destroyWaline = ele => ele.destroy()

  const initWaline = (Fn, el = document, path = window.location.pathname) => {
    const waline = Fn({
      el: el.querySelector('#waline-wrap'),
      serverURL: 'https://dream-lime.vercel.app',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      comment: false,
      ...option,
      path: isShuoshuo ? path : (option && option.path) || path
    })

    if (isShuoshuo) {
      window.shuoshuoComment.destroyWaline = () => {
        destroyWaline(waline)
        if (el.children.length) {
          el.innerHTML = ''
          el.classList.add('no-comment')
        }
      }
    }
  }

  const loadWaline = (el, path) => {
    if (initFn) initWaline(initFn, el, path)
    else {
      btf.getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css')
        .then(() => import('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js'))
        .then(({ init }) => {
          initFn = init || Waline.init
          initWaline(initFn, el, path)
          window.walineFn = initFn
        })
    }
  }

  if (isShuoshuo) {
    'Waline' === 'Waline'
      ? window.shuoshuoComment = { loadComment: loadWaline } 
      : window.loadOtherComment = loadWaline
    return
  }

  if ('Waline' === 'Waline' || !true) {
    if (true) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
    else setTimeout(loadWaline, 0)
  } else {
    window.loadOtherComment = loadWaline
  }
})()</script></div><div class="aplayer no-destroy" data-id="9430428607" data-server="tencent" data-type="playlist" data-fixed="true" data-autoplay="false"> </div><script defer src="https://cdn.jsdelivr.net/npm/sweetalert2@latest/dist/sweetalert2.all.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script>(() => {
  const destroyAplayer = () => {
    if (window.aplayers) {
      for (let i = 0; i < window.aplayers.length; i++) {
        if (!window.aplayers[i].options.fixed) {
          window.aplayers[i].destroy()
        }
      }
    }
  }

  const runMetingJS = () => {
    typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()
  }

  btf.addGlobalFn('pjaxSend', destroyAplayer, 'destroyAplayer')
  btf.addGlobalFn('pjaxComplete', loadMeting, 'runMetingJS')
})()</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>(() => {
  const pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show","#web_bg",".js-pjax"]

  window.pjax = new Pjax({
    elements: 'a:not([target="_blank"])',
    selectors: pjaxSelectors,
    cacheBust: false,
    analytics: false,
    scrollRestoration: false
  })

  const triggerPjaxFn = (val) => {
    if (!val) return
    Object.values(val).forEach(fn => fn())
  }

  document.addEventListener('pjax:send', () => {
    // removeEventListener
    btf.removeGlobalFnEvent('pjaxSendOnce')
    btf.removeGlobalFnEvent('themeChange')

    // reset readmode
    const $bodyClassList = document.body.classList
    if ($bodyClassList.contains('read-mode')) $bodyClassList.remove('read-mode')

    triggerPjaxFn(window.globalFn.pjaxSend)
  })

  document.addEventListener('pjax:complete', () => {
    btf.removeGlobalFnEvent('pjaxCompleteOnce')
    document.querySelectorAll('script[data-pjax]').forEach(item => {
      const newScript = document.createElement('script')
      const content = item.text || item.textContent || item.innerHTML || ""
      Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
      newScript.appendChild(document.createTextNode(content))
      item.parentNode.replaceChild(newScript, item)
    })

    triggerPjaxFn(window.globalFn.pjaxComplete)
  })

  document.addEventListener('pjax:error', e => {
    if (e.request.status === 404) {
      const usePjax = true
      false 
        ? (usePjax ? pjax.loadUrl('/404.html') : window.location.href = '/404.html')
        : window.location.href = e.request.responseURL
    }
  })
})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>